{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6d7c171",
   "metadata": {},
   "source": [
    "# Project 1 · BASIC: CIFAR-10 / CIFAR-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "72c443f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math, numpy as np, torch, torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightly.transforms import MAETransform\n",
    "from lightly.models import utils\n",
    "from timm.models.vision_transformer import vit_base_patch32_224\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "from lightly.models.modules.masked_vision_transformer_timm import MaskedVisionTransformerTIMM\n",
    "from lightly.models.modules.masked_autoencoder_timm import MAEDecoderTIMM\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c8094a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b487117e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7955d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "87124a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cifar_stats(name=\"cifar100\", root=\"data\", batch_size=5000):\n",
    "    ds_class = getattr(torchvision.datasets, name.upper())\n",
    "    ds = ds_class(root, train=True, download=True, transform=T.ToTensor())\n",
    "    loader = DataLoader(ds, batch_size=batch_size, num_workers=2, shuffle=False)\n",
    "    ch_sum = torch.zeros(3)\n",
    "    ch_sum_sq = torch.zeros(3)\n",
    "    n_pixels = 0\n",
    "    for imgs, _ in loader:\n",
    "        b, c, h, w = imgs.shape\n",
    "        n_pixels += b * h * w\n",
    "        ch_sum    += imgs.sum(dim=[0,2,3])\n",
    "        ch_sum_sq += (imgs**2).sum(dim=[0,2,3])\n",
    "    mean = ch_sum / n_pixels\n",
    "    std  = torch.sqrt(ch_sum_sq / n_pixels - mean**2)\n",
    "    return mean.tolist(), std.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0c527619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets(root=\"data\", val_size=5_000, img_size=224, seed=42):\n",
    "    mean, std = compute_cifar_stats(name=\"cifar100\", root=root)\n",
    "    ssl_tfm  = MAETransform(input_size=img_size, min_scale=0.2, normalize={\"mean\": mean, \"std\": std})\n",
    "    eval_tfm = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(img_size + 10),\n",
    "        torchvision.transforms.CenterCrop(img_size),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    base = torchvision.datasets.CIFAR100(root, train=True, download=True, transform=eval_tfm)\n",
    "    targets = np.array(base.targets)\n",
    "    idx = np.arange(len(base))\n",
    "    tr_idx, val_idx = train_test_split(idx, test_size=val_size, stratify=targets, random_state=seed)\n",
    "\n",
    "    train_ssl = Subset(torchvision.datasets.CIFAR100(root, train=True, transform=ssl_tfm, download=False), tr_idx)\n",
    "    val_set   = Subset(torchvision.datasets.CIFAR100(root, train=True, transform=eval_tfm, download=False), val_idx)\n",
    "    test_set  = torchvision.datasets.CIFAR100(root, train=False, transform=eval_tfm, download=True)\n",
    "    return train_ssl, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f6bdd208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAE(pl.LightningModule):\n",
    "    def __init__(self, lr=1.5e-4):\n",
    "        super().__init__()\n",
    "        decoder_dim = 512\n",
    "        vit         = vit_base_patch32_224()\n",
    "        self.mask_ratio   = 0.75\n",
    "        self.patch_size   = vit.patch_embed.patch_size[0]\n",
    "        self.backbone     = MaskedVisionTransformerTIMM(vit=vit)\n",
    "        self.sequence_len = self.backbone.sequence_length\n",
    "        self.decoder      = MAEDecoderTIMM(\n",
    "            num_patches=vit.patch_embed.num_patches,\n",
    "            patch_size=self.patch_size,\n",
    "            embed_dim=vit.embed_dim,\n",
    "            decoder_embed_dim=decoder_dim,\n",
    "            decoder_depth=1,\n",
    "            decoder_num_heads=16,\n",
    "            mlp_ratio=4.0,\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward_encoder(self, imgs, idx_keep=None):\n",
    "        return self.backbone.encode(images=imgs, idx_keep=idx_keep)\n",
    "\n",
    "    def forward_decoder(self, x_enc, idx_keep, idx_mask):\n",
    "        b = x_enc.size(0)\n",
    "        x_dec = self.decoder.embed(x_enc)\n",
    "        x_masked = utils.repeat_token(self.decoder.mask_token, (b, self.sequence_len))\n",
    "        x_masked = utils.set_at_index(x_masked, idx_keep, x_dec.type_as(x_masked))\n",
    "        x_decoded = self.decoder.decode(x_masked)\n",
    "        x_pred = utils.get_at_index(x_decoded, idx_mask)\n",
    "        return self.decoder.predict(x_pred)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        views, _ = batch\n",
    "        imgs = views[0]\n",
    "        bsz = imgs.size(0)\n",
    "        idx_keep, idx_mask = utils.random_token_mask(\n",
    "            size=(bsz, self.sequence_len),\n",
    "            mask_ratio=self.mask_ratio,\n",
    "            device=imgs.device,\n",
    "        )\n",
    "        x_enc = self.forward_encoder(imgs, idx_keep)\n",
    "        x_pred = self.forward_decoder(x_enc, idx_keep, idx_mask)\n",
    "\n",
    "        patches = utils.patchify(imgs, self.patch_size)\n",
    "        target  = utils.get_at_index(patches, idx_mask - 1)\n",
    "        loss = self.criterion(x_pred, target)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e3fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repos\\Warsztaty_Badawcze\\env\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                        | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | backbone  | MaskedVisionTransformerTIMM | 88.2 M | train\n",
      "1 | decoder   | MAEDecoderTIMM              | 5.1 M  | train\n",
      "2 | criterion | MSELoss                     | 0      | train\n",
      "------------------------------------------------------------------\n",
      "93.3 M    Trainable params\n",
      "64.0 K    Non-trainable params\n",
      "93.4 M    Total params\n",
      "373.497   Total estimated model params size (MB)\n",
      "292       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "d:\\Repos\\Warsztaty_Badawcze\\env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  39%|███▉      | 68/175 [03:39<05:44,  0.31it/s, v_num=7, train_loss_step=0.280, train_loss_epoch=0.350] "
     ]
    }
   ],
   "source": [
    "train_ssl, val_set, test_set = make_datasets()\n",
    "train_loader = DataLoader(\n",
    "    train_ssl,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False,\n",
    "    prefetch_factor=2,\n",
    ")\n",
    "\n",
    "model = MAE()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=80,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\",\n",
    "    enable_checkpointing=False,\n",
    "    log_every_n_steps=50,\n",
    ")\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf4d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embs(mod, dataset):\n",
    "    loader = DataLoader(\n",
    "        dataset, batch_size=256, shuffle=False,\n",
    "        num_workers=os.cpu_count() // 2, pin_memory=True,\n",
    "        persistent_workers=True, prefetch_factor=4,\n",
    "    )\n",
    "    embs, lbls = [], []\n",
    "    mod.eval()\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(mod.device, non_blocking=True)\n",
    "            feat, _ = mod.backbone.encode(images=imgs)\n",
    "            embs.append(feat.cpu())\n",
    "            lbls.append(labels)\n",
    "    return torch.vstack(embs).numpy(), torch.hstack(lbls).numpy()\n",
    "\n",
    "train_embs, train_lbls = extract_embs(model, val_set)\n",
    "test_embs,  test_lbls  = extract_embs(model, test_set)\n",
    "\n",
    "lin_clf = LogisticRegression(max_iter=500).fit(train_embs, train_lbls)\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=20).fit(train_embs, train_lbls)\n",
    "print(f\"Linear probe accuracy: {lin_clf.score(test_embs, test_lbls):.4f}\")\n",
    "print(f\"k-NN            acc.: {knn_clf.score(test_embs, test_lbls):.4f}\")\n",
    "\n",
    "def plot_2d(emb, lbl, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=emb[:, 0], y=emb[:, 1], hue=lbl, palette=\"tab10\", s=10, linewidth=0)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"best\", bbox_to_anchor=(1, 1), ncol=2, fontsize=\"x-small\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "pca_emb = PCA(n_components=2).fit_transform(test_embs)\n",
    "plot_2d(pca_emb, test_lbls, \"PCA of MAE embeddings\")\n",
    "\n",
    "tsne_emb = TSNE(n_components=2, random_state=42).fit_transform(test_embs)\n",
    "plot_2d(tsne_emb, test_lbls, \"t-SNE of MAE embeddings\")\n",
    "\n",
    "umap_emb = umap.UMAP(random_state=42).fit_transform(test_embs)\n",
    "plot_2d(umap_emb, test_lbls, \"UMAP of MAE embeddings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
