{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-02T14:51:44.105784Z",
     "start_time": "2025-06-02T14:51:44.103791Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms as T\n",
    "from lightly.transforms import SimCLRTransform, DINOTransform, MAETransform, MoCoV2Transform, utils\n",
    "from datasets import create_dataset, create_bootstrap_dataloader\n",
    "from models import *\n",
    "from tqdm import tqdm\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import copy\n",
    "import gc\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:53:26.696332Z",
     "start_time": "2025-06-02T14:53:26.668564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "ef752e8792145f94",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Losowość",
   "id": "fd9c78d9361bd984"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:49:08.560433Z",
     "start_time": "2025-06-02T13:49:08.556485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed: int=42):\n",
    "    pl.seed_everything(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    ####### Normaly you would also need to seed those generators but `pytorch_lightning` does it in one func\n",
    "    # random.seed(seed)\n",
    "    # np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    ######\n",
    "    torch.cuda.manual_seed(seed) # Don't know if pytorch lightning does this\n",
    "    torch.cuda.manual_seed_all(seed) # Don't know if pytorch lightning does this\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ],
   "id": "bce35f10f9cf2cd5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transformacje",
   "id": "169b44531963bc15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T13:49:09.950561Z",
     "start_time": "2025-06-02T13:49:09.947379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_transform = T.v2.Compose(\n",
    "    [\n",
    "        T.Resize((224, 224)),\n",
    "        T.v2.ToImage(),\n",
    "        T.v2.ToDtype(torch.float32, scale=True),\n",
    "        T.Normalize(\n",
    "            mean=utils.IMAGENET_NORMALIZE[\"mean\"],\n",
    "            std=utils.IMAGENET_NORMALIZE[\"std\"],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "scratch_transform = T.v2.Compose(\n",
    "    [\n",
    "        T.RandomResizedCrop((224, 224)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.v2.ToImage(),\n",
    "        T.v2.ToDtype(torch.float32, scale=True),\n",
    "        T.Normalize(\n",
    "            mean=utils.IMAGENET_NORMALIZE[\"mean\"],\n",
    "            std=utils.IMAGENET_NORMALIZE[\"std\"],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "byol_transform = SimCLRTransform(input_size=(224, 224), vf_prob=0.5, rr_prob=0.5) #BYOL używa SimCLR-owych transformacji"
   ],
   "id": "31c97f2daf1d3b54",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hussein/pytoniec/lib/python3.13/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Datasety",
   "id": "d16534c4e56f5138"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:34:41.886407Z",
     "start_time": "2025-06-02T13:49:13.565359Z"
    }
   },
   "cell_type": "code",
   "source": "train_full_ds, train_ssl_ds, train_ds, test_ds = create_dataset(\"ImageNet1K\", 0.9, byol_transform, scratch_transform, test_transform, \"data\", False)",
   "id": "7be27ff916e02c1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of entire train dataset:  1281167\n",
      "Length of SSL train dataset:  1153050\n",
      "Length of classification train dataset:  128117\n",
      "Length of test dataset:  50000\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hiperparametry",
   "id": "55ad25e76276099b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:14:59.256680Z",
     "start_time": "2025-06-02T15:14:59.230483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### PARAMETERS ###\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_WORKERS = 3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "#GARBAGE COLLECTOR FAJNA SPRAWA - BEZ NIEGO VRAMu BRAKUJE\n",
    "if device == \"gpu\":\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ],
   "id": "4bbf1c068772ebe0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Uczenie na resnet18 (bootstrapowe datasety pragnę zauważyć, stratified znaczy, że stara się balansować próbkowanie po klasach)",
   "id": "a51168d1b5a39577"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:41:21.742709Z",
     "start_time": "2025-06-02T14:39:43.441322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# jednak nie będzie stratified bo ekstrakcja tego ma zając 2,5h xd ( te targety są wyliczane w funkcji create_dataset, można ją zmodyfikować tak aby zwracała\n",
    "labels = np.array([y for _, y in tqdm(train_ssl_ds)])"
   ],
   "id": "6d8a19cf7d1098b6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12497/1153050 [01:37<2:29:03, 127.53it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m labels \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(\u001B[43m[\u001B[49m\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_ssl_ds\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m)\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/tqdm/std.py:1181\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1178\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1180\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1181\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m   1182\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[1;32m   1183\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[1;32m   1184\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/torch/utils/data/dataset.py:408\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    406\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    407\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[0;32m--> 408\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/torchvision/datasets/folder.py:247\u001B[0m, in \u001B[0;36mDatasetFolder.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    245\u001B[0m sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader(path)\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 247\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    249\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/lightly/transforms/multi_view_transform.py:34\u001B[0m, in \u001B[0;36mMultiViewTransform.__call__\u001B[0;34m(self, image)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, image: Union[Tensor, Image]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[List[Tensor], List[Image]]:\n\u001B[1;32m     22\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Transforms an image into multiple views.\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \n\u001B[1;32m     24\u001B[0m \u001B[38;5;124;03m    Every transform in self.transforms creates a new view.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     32\u001B[0m \n\u001B[1;32m     33\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 34\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m transform \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms]\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/lightly/transforms/simclr_transform.py:183\u001B[0m, in \u001B[0;36mSimCLRViewTransform.__call__\u001B[0;34m(self, image)\u001B[0m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, image: Union[Tensor, Image]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m    172\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;124;03m    Applies the transforms to the input image.\u001B[39;00m\n\u001B[1;32m    174\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    181\u001B[0m \n\u001B[1;32m    182\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 183\u001B[0m     transformed: Tensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    184\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m transformed\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/torchvision/transforms/v2/_container.py:51\u001B[0m, in \u001B[0;36mCompose.forward\u001B[0;34m(self, *inputs)\u001B[0m\n\u001B[1;32m     49\u001B[0m needs_unpacking \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(inputs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m transform \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m---> 51\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     52\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m outputs \u001B[38;5;28;01mif\u001B[39;00m needs_unpacking \u001B[38;5;28;01melse\u001B[39;00m (outputs,)\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/torchvision/transforms/v2/_container.py:105\u001B[0m, in \u001B[0;36mRandomApply.forward\u001B[0;34m(self, *inputs)\u001B[0m\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inputs \u001B[38;5;28;01mif\u001B[39;00m needs_unpacking \u001B[38;5;28;01melse\u001B[39;00m inputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m transform \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m--> 105\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    106\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m outputs \u001B[38;5;28;01mif\u001B[39;00m needs_unpacking \u001B[38;5;28;01melse\u001B[39;00m (outputs,)\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/torchvision/transforms/v2/_transform.py:69\u001B[0m, in \u001B[0;36mTransform.forward\u001B[0;34m(self, *inputs)\u001B[0m\n\u001B[1;32m     63\u001B[0m needs_transform_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_needs_transform_list(flat_inputs)\n\u001B[1;32m     64\u001B[0m params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_params(\n\u001B[1;32m     65\u001B[0m     [inpt \u001B[38;5;28;01mfor\u001B[39;00m (inpt, needs_transform) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(flat_inputs, needs_transform_list) \u001B[38;5;28;01mif\u001B[39;00m needs_transform]\n\u001B[1;32m     66\u001B[0m )\n\u001B[1;32m     68\u001B[0m flat_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m---> 69\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43minpt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m needs_transform \u001B[38;5;28;01melse\u001B[39;00m inpt\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (inpt, needs_transform) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(flat_inputs, needs_transform_list)\n\u001B[1;32m     71\u001B[0m ]\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tree_unflatten(flat_outputs, spec)\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/torchvision/transforms/v2/_color.py:163\u001B[0m, in \u001B[0;36mColorJitter.transform\u001B[0;34m(self, inpt, params)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fn_id \u001B[38;5;129;01min\u001B[39;00m params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfn_idx\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m fn_id \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m brightness_factor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 163\u001B[0m         output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_kernel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madjust_brightness\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbrightness_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbrightness_factor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m fn_id \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m contrast_factor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    165\u001B[0m         output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_kernel(F\u001B[38;5;241m.\u001B[39madjust_contrast, output, contrast_factor\u001B[38;5;241m=\u001B[39mcontrast_factor)\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/torchvision/transforms/v2/_transform.py:49\u001B[0m, in \u001B[0;36mTransform._call_kernel\u001B[0;34m(self, functional, inpt, *args, **kwargs)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_call_kernel\u001B[39m(\u001B[38;5;28mself\u001B[39m, functional: Callable, inpt: Any, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m     48\u001B[0m     kernel \u001B[38;5;241m=\u001B[39m _get_kernel(functional, \u001B[38;5;28mtype\u001B[39m(inpt), allow_passthrough\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mkernel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minpt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/torchvision/transforms/v2/functional/_color.py:132\u001B[0m, in \u001B[0;36m_adjust_brightness_image_pil\u001B[0;34m(image, brightness_factor)\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;129m@_register_kernel_internal\u001B[39m(adjust_brightness, PIL\u001B[38;5;241m.\u001B[39mImage\u001B[38;5;241m.\u001B[39mImage)\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_adjust_brightness_image_pil\u001B[39m(image: PIL\u001B[38;5;241m.\u001B[39mImage\u001B[38;5;241m.\u001B[39mImage, brightness_factor: \u001B[38;5;28mfloat\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m PIL\u001B[38;5;241m.\u001B[39mImage\u001B[38;5;241m.\u001B[39mImage:\n\u001B[0;32m--> 132\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_FP\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madjust_brightness\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbrightness_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbrightness_factor\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/torchvision/transforms/_functional_pil.py:73\u001B[0m, in \u001B[0;36madjust_brightness\u001B[0;34m(img, brightness_factor)\u001B[0m\n\u001B[1;32m     70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimg should be PIL Image. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(img)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     72\u001B[0m enhancer \u001B[38;5;241m=\u001B[39m ImageEnhance\u001B[38;5;241m.\u001B[39mBrightness(img)\n\u001B[0;32m---> 73\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43menhancer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menhance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbrightness_factor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/PIL/ImageEnhance.py:40\u001B[0m, in \u001B[0;36m_Enhance.enhance\u001B[0;34m(self, factor)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21menhance\u001B[39m(\u001B[38;5;28mself\u001B[39m, factor: \u001B[38;5;28mfloat\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Image\u001B[38;5;241m.\u001B[39mImage:\n\u001B[1;32m     30\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;124;03m    Returns an enhanced image.\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;03m    :rtype: :py:class:`~PIL.Image.Image`\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdegenerate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfactor\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/PIL/Image.py:3574\u001B[0m, in \u001B[0;36mblend\u001B[0;34m(im1, im2, alpha)\u001B[0m\n\u001B[1;32m   3572\u001B[0m im1\u001B[38;5;241m.\u001B[39mload()\n\u001B[1;32m   3573\u001B[0m im2\u001B[38;5;241m.\u001B[39mload()\n\u001B[0;32m-> 3574\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m im1\u001B[38;5;241m.\u001B[39m_new(\u001B[43mcore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mim1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mim2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Uczenie na pustym resnet18",
   "id": "6b637cd29d2d6fac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1e53e95285d85006"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Uczenie na jakiejś prostej MLP",
   "id": "4445bdb21f49d320"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:11:00.665787Z",
     "start_time": "2025-06-02T15:16:43.271012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dl_train = create_bootstrap_dataloader(\n",
    "    dataset=train_ssl_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    random_state=SEED,\n",
    "\n",
    "    pin_memory=True,\n",
    "    drop_last=True)\n",
    "dl_val = torch.utils.data.DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "#backbone_type - simple_mlp - nie używamy resneta\n",
    "dino_model = BYOLModel(lr=LEARNING_RATE, max_epochs=NUM_EPOCHS, backbone_type='simple_mlp')\n",
    "\n",
    "trainer = pl.Trainer(accelerator='cuda', devices=1, max_epochs=NUM_EPOCHS, precision=\"16-mixed\", log_every_n_steps=1)\n",
    "\n",
    "trainer.fit(dino_model, dl_train, dl_val)"
   ],
   "id": "b18b65378d4033cf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type                 | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | online_backbone   | SimpleMLP            | 19.3 M | train\n",
      "1 | online_projection | SimCLRProjectionHead | 33.3 K | train\n",
      "2 | online_predictor  | Sequential           | 16.7 K | train\n",
      "3 | target_backbone   | SimpleMLP            | 19.3 M | train\n",
      "4 | target_projection | SimCLRProjectionHead | 33.3 K | train\n",
      "-------------------------------------------------------------------\n",
      "19.4 M    Trainable params\n",
      "19.3 M    Non-trainable params\n",
      "38.7 M    Total params\n",
      "154.743   Total estimated model params size (MB)\n",
      "43        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61e2a57d6ad44b2aad015efd2026831f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be271c4d51384d20b52ca037ee8ed35e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b24138be6804a88af8c7468fa752873"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ddfdca47e10cd6e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# zostawiam output poniższej komórki, wydaje mi się zabawny",
   "id": "a048bb87ec1fe8c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:13:10.884517Z",
     "start_time": "2025-06-02T14:55:59.194967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dl_train = create_bootstrap_dataloader(\n",
    "    dataset=train_ssl_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    random_state=SEED,\n",
    "\n",
    "    pin_memory=True,\n",
    "    drop_last=True)\n",
    "dl_val = torch.utils.data.DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "#backbone_type - simple_mlp - nie używamy resneta\n",
    "dino_model = BYOLModel(lr=LEARNING_RATE, max_epochs=NUM_EPOCHS, backbone_type='simple_mlp')\n",
    "\n",
    "trainer = pl.Trainer(accelerator='cuda', devices=1, max_epochs=NUM_EPOCHS, precision=\"16-mixed\", log_every_n_steps=1)\n",
    "\n",
    "trainer.fit(dino_model, dl_train, dl_val)"
   ],
   "id": "9d3b953af2899ce4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type                 | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | online_backbone   | SimpleMLP            | 310 M  | train\n",
      "1 | online_projection | SimCLRProjectionHead | 33.3 K | train\n",
      "2 | online_predictor  | Sequential           | 16.7 K | train\n",
      "3 | target_backbone   | SimpleMLP            | 310 M  | train\n",
      "4 | target_projection | SimCLRProjectionHead | 33.3 K | train\n",
      "-------------------------------------------------------------------\n",
      "311 M     Trainable params\n",
      "311 M     Non-trainable params\n",
      "622 M     Total params\n",
      "2,488.167 Total estimated model params size (MB)\n",
      "51        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63f2735988164644852232ccd9579dd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "478ec23b0c904880b10377f3630a6821"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/pytorch_lightning/trainer/call.py:48\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     47\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 48\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:599\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    593\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    594\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    595\u001B[0m     ckpt_path,\n\u001B[1;32m    596\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    597\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    598\u001B[0m )\n\u001B[0;32m--> 599\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    601\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1009\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m   1010\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m   1011\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m-> 1012\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1014\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:1056\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1055\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m-> 1056\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1057\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001B[0m, in \u001B[0;36m_FitLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start()\n\u001B[0;32m--> 216\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001B[0m, in \u001B[0;36m_FitLoop.advance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    454\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 455\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/pytorch_lightning/loops/training_epoch_loop.py:150\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.run\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end(data_fetcher)\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/pytorch_lightning/loops/training_epoch_loop.py:339\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.advance\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    337\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mis_last_batch \u001B[38;5;241m=\u001B[39m data_fetcher\u001B[38;5;241m.\u001B[39mdone\n\u001B[0;32m--> 339\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_callback_hooks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mon_train_batch_end\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    340\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_lightning_module_hook(trainer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_train_batch_end\u001B[39m\u001B[38;5;124m\"\u001B[39m, batch_output, batch, batch_idx)\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/pytorch_lightning/trainer/call.py:227\u001B[0m, in \u001B[0;36m_call_callback_hooks\u001B[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001B[0m\n\u001B[1;32m    226\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Callback]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcallback\u001B[38;5;241m.\u001B[39mstate_key\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 227\u001B[0m             \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pl_module:\n\u001B[1;32m    230\u001B[0m     \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/pytorch_lightning/callbacks/progress/tqdm_progress.py:279\u001B[0m, in \u001B[0;36mTQDMProgressBar.on_train_batch_end\u001B[0;34m(self, trainer, pl_module, outputs, batch, batch_idx)\u001B[0m\n\u001B[1;32m    278\u001B[0m _update_n(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_progress_bar, n)\n\u001B[0;32m--> 279\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_progress_bar\u001B[38;5;241m.\u001B[39mset_postfix(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpl_module\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/pytorch_lightning/callbacks/progress/progress_bar.py:198\u001B[0m, in \u001B[0;36mProgressBar.get_metrics\u001B[0;34m(self, trainer, pl_module)\u001B[0m\n\u001B[1;32m    197\u001B[0m standard_metrics \u001B[38;5;241m=\u001B[39m get_standard_metrics(trainer)\n\u001B[0;32m--> 198\u001B[0m pbar_metrics \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprogress_bar_metrics\u001B[49m\n\u001B[1;32m    199\u001B[0m duplicates \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(standard_metrics\u001B[38;5;241m.\u001B[39mkeys() \u001B[38;5;241m&\u001B[39m pbar_metrics\u001B[38;5;241m.\u001B[39mkeys())\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:1667\u001B[0m, in \u001B[0;36mTrainer.progress_bar_metrics\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1661\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"The metrics sent to the progress bar.\u001B[39;00m\n\u001B[1;32m   1662\u001B[0m \n\u001B[1;32m   1663\u001B[0m \u001B[38;5;124;03mThis includes metrics logged via :meth:`~pytorch_lightning.core.LightningModule.log` with the\u001B[39;00m\n\u001B[1;32m   1664\u001B[0m \u001B[38;5;124;03m:paramref:`~pytorch_lightning.core.LightningModule.log.prog_bar` argument set.\u001B[39;00m\n\u001B[1;32m   1665\u001B[0m \n\u001B[1;32m   1666\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1667\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_logger_connector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprogress_bar_metrics\u001B[49m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:254\u001B[0m, in \u001B[0;36m_LoggerConnector.progress_bar_metrics\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    253\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_results:\n\u001B[0;32m--> 254\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetrics\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpbar\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_bar_metrics\u001B[38;5;241m.\u001B[39mupdate(metrics)\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:235\u001B[0m, in \u001B[0;36m_LoggerConnector.metrics\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_results \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 235\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_results\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43mon_step\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:493\u001B[0m, in \u001B[0;36m_ResultCollection.metrics\u001B[0;34m(self, on_step)\u001B[0m\n\u001B[1;32m    492\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result_metric\u001B[38;5;241m.\u001B[39mmeta\u001B[38;5;241m.\u001B[39mprog_bar:\n\u001B[0;32m--> 493\u001B[0m         metrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpbar\u001B[39m\u001B[38;5;124m\"\u001B[39m][forked_name] \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_tensors_to_scalars\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    495\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m metrics\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/lightning_fabric/utilities/apply_func.py:136\u001B[0m, in \u001B[0;36mconvert_tensors_to_scalars\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m value\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m--> 136\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mapply_to_collection\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mto_item\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/lightning_utilities/core/apply_func.py:66\u001B[0m, in \u001B[0;36mapply_to_collection\u001B[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, dtype):  \u001B[38;5;66;03m# single element\u001B[39;00m\n\u001B[0;32m---> 66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m data\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mlist\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, dtype) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m data):  \u001B[38;5;66;03m# 1d homogeneous list\u001B[39;00m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/lightning_fabric/utilities/apply_func.py:134\u001B[0m, in \u001B[0;36mconvert_tensors_to_scalars.<locals>.to_item\u001B[0;34m(value)\u001B[0m\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    132\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe metric `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvalue\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` does not contain a single element, thus it cannot be converted to a scalar.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    133\u001B[0m     )\n\u001B[0;32m--> 134\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mvalue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 21\u001B[0m\n\u001B[1;32m     17\u001B[0m dino_model \u001B[38;5;241m=\u001B[39m BYOLModel(lr\u001B[38;5;241m=\u001B[39mLEARNING_RATE, max_epochs\u001B[38;5;241m=\u001B[39mNUM_EPOCHS, backbone_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msimple_mlp\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     19\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pl\u001B[38;5;241m.\u001B[39mTrainer(accelerator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m, devices\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, max_epochs\u001B[38;5;241m=\u001B[39mNUM_EPOCHS, precision\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m16-mixed\u001B[39m\u001B[38;5;124m\"\u001B[39m, log_every_n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 21\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdino_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdl_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdl_val\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/pytorch_lightning/trainer/trainer.py:561\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    560\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshould_stop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m--> 561\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    562\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    563\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/pytoniec/lib/python3.13/site-packages/pytorch_lightning/trainer/call.py:65\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(launcher, _SubprocessScriptLauncher):\n\u001B[1;32m     64\u001B[0m         launcher\u001B[38;5;241m.\u001B[39mkill(_get_sigkill_signal())\n\u001B[0;32m---> 65\u001B[0m     \u001B[43mexit\u001B[49m(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n\u001B[1;32m     68\u001B[0m     _interrupt(trainer, exception)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'exit' is not defined"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a0f872244466592d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
