{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbfc6a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n",
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from lightly.transforms import SimCLRTransform, DINOTransform, MAETransform, MoCoV2Transform, utils\n",
    "from datasets import create_dataset\n",
    "from models import SimCLRModel\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import copy\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817f4c8a",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4586eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed: int=42):\n",
    "    pl.seed_everything(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    ####### Normaly you would also need to seed those generators but `pytorch_lightning` does it in one func\n",
    "    # random.seed(seed)\n",
    "    # np.random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    ######\n",
    "    torch.cuda.manual_seed(seed) # Don't know if pytorch lightning does this\n",
    "    torch.cuda.manual_seed_all(seed) # Don't know if pytorch lightning does this\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f5de75",
   "metadata": {},
   "source": [
    "# Transformacje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00cffd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_transform = T.v2.Compose(\n",
    "    [\n",
    "        T.Resize((224, 224)),\n",
    "        T.v2.ToImage(),\n",
    "        T.v2.ToDtype(torch.float32, scale=True),\n",
    "        T.Normalize(\n",
    "            mean=utils.IMAGENET_NORMALIZE[\"mean\"],\n",
    "            std=utils.IMAGENET_NORMALIZE[\"std\"],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "scratch_transform = T.v2.Compose(\n",
    "    [\n",
    "        T.RandomResizedCrop((224, 224)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.v2.ToImage(),\n",
    "        T.v2.ToDtype(torch.float32, scale=True),\n",
    "        T.Normalize(\n",
    "            mean=utils.IMAGENET_NORMALIZE[\"mean\"],\n",
    "            std=utils.IMAGENET_NORMALIZE[\"std\"],\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "simclr_transform = SimCLRTransform(\n",
    "    input_size=(224, 224),\n",
    "    vf_prob=0.5,\n",
    "    rr_prob=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2d472a",
   "metadata": {},
   "source": [
    "# Zbiorki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6901f5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of entire train dataset:  50000\n",
      "Length of SSL train dataset:  45000\n",
      "Length of classification train dataset:  5000\n",
      "Length of test dataset:  10000\n"
     ]
    }
   ],
   "source": [
    "train_full_cifar10_simclr, train_ssl_cifar10_simclr, train_cifar10_simclr, test_cifar10_simclr = create_dataset(\"CIFAR10\", 0.9, simclr_transform, scratch_transform, test_transform, \"data\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3d4b5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of entire train dataset:  50000\n",
      "Length of SSL train dataset:  45000\n",
      "Length of classification train dataset:  5000\n",
      "Length of test dataset:  10000\n"
     ]
    }
   ],
   "source": [
    "train_full_cifar100_simclr, train_ssl_cifar100_simclr, train_cifar100_simclr, test_cifar100_simclr = create_dataset(\"CIFAR100\", 0.9, simclr_transform, scratch_transform, test_transform, \"data\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73ba92c",
   "metadata": {},
   "source": [
    "# Hiperparametry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1ce9fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: gpu\n"
     ]
    }
   ],
   "source": [
    "### PARAMETERS ###\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = [0.001, 0.01]\n",
    "BACKBONE_TYPE = ['pretrained', 'random']\n",
    "NUM_WORKERS = 3\n",
    "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "#GARBAGE COLLECTOR FAJNA SPRAWA - BEZ NIEGO VRAMu BRAKUJE\n",
    "if device == \"gpu\":\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7243f3ea",
   "metadata": {},
   "source": [
    "# CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a87de",
   "metadata": {},
   "source": [
    "## Dataloadery i trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff85dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train_cifar10_simclr = torch.utils.data.DataLoader(\n",
    "    train_ssl_cifar10_simclr,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "dl_val_cifar10_simclr = torch.utils.data.DataLoader(\n",
    "    test_cifar10_simclr,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f218408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "import os\n",
    "\n",
    "class SaveAtEpochsCallback(Callback):\n",
    "    def __init__(self, save_epochs, dirpath=\"checkpoints\"):\n",
    "        super().__init__()\n",
    "        self.save_epochs = set(save_epochs)\n",
    "        self.dirpath = dirpath\n",
    "        os.makedirs(self.dirpath, exist_ok=True)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        current_epoch = trainer.current_epoch  # epoka 0-based\n",
    "        if current_epoch + 1 in self.save_epochs:\n",
    "            filename = f\"model_epoch_{current_epoch}.ckpt\"\n",
    "            path = os.path.join(self.dirpath, filename)\n",
    "            trainer.save_checkpoint(path)\n",
    "            print(f\"Zapisano model po epoce {current_epoch}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4e0acec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M | train\n",
      "1 | projection_head | SimCLRProjectionHead | 328 K  | train\n",
      "2 | criterion       | NTXentLoss           | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "46.022    Total estimated model params size (MB)\n",
      "77        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 351/351 [03:35<00:00,  1.63it/s, v_num=1, train_loss=4.200, val_loss=3.760, val_ari=0.176, val_nmi=0.287, train_ari=0.145, train_nmi=0.223] Zapisano model po epoce 9: checkpoints/simclr_cifar10_pretrained_0.001\\model_epoch_9.ckpt\n",
      "Epoch 14: 100%|██████████| 351/351 [03:32<00:00,  1.65it/s, v_num=1, train_loss=4.170, val_loss=3.740, val_ari=0.190, val_nmi=0.302, train_ari=0.167, train_nmi=0.257]Zapisano model po epoce 14: checkpoints/simclr_cifar10_pretrained_0.001\\model_epoch_14.ckpt\n",
      "Epoch 19: 100%|██████████| 351/351 [03:33<00:00,  1.64it/s, v_num=1, train_loss=4.130, val_loss=3.730, val_ari=0.193, val_nmi=0.299, train_ari=0.178, train_nmi=0.275]Zapisano model po epoce 19: checkpoints/simclr_cifar10_pretrained_0.001\\model_epoch_19.ckpt\n",
      "Epoch 19: 100%|██████████| 351/351 [03:34<00:00,  1.64it/s, v_num=1, train_loss=4.130, val_loss=3.730, val_ari=0.193, val_nmi=0.299, train_ari=0.178, train_nmi=0.275]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 351/351 [03:37<00:00,  1.61it/s, v_num=1, train_loss=4.130, val_loss=3.730, val_ari=0.193, val_nmi=0.299, train_ari=0.178, train_nmi=0.275]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M | train\n",
      "1 | projection_head | SimCLRProjectionHead | 328 K  | train\n",
      "2 | criterion       | NTXentLoss           | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "46.022    Total estimated model params size (MB)\n",
      "77        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 351/351 [03:55<00:00,  1.49it/s, v_num=0, train_loss=4.460, val_loss=3.890, val_ari=0.0853, val_nmi=0.143, train_ari=0.0704, train_nmi=0.117] Zapisano model po epoce 9: checkpoints/simclr_cifar10_random_0.001\\model_epoch_9.ckpt\n",
      "Epoch 14: 100%|██████████| 351/351 [03:56<00:00,  1.49it/s, v_num=0, train_loss=4.420, val_loss=3.810, val_ari=0.0936, val_nmi=0.160, train_ari=0.0825, train_nmi=0.137]Zapisano model po epoce 14: checkpoints/simclr_cifar10_random_0.001\\model_epoch_14.ckpt\n",
      "Epoch 19: 100%|██████████| 351/351 [03:56<00:00,  1.48it/s, v_num=0, train_loss=4.290, val_loss=3.790, val_ari=0.101, val_nmi=0.171, train_ari=0.092, train_nmi=0.152]  Zapisano model po epoce 19: checkpoints/simclr_cifar10_random_0.001\\model_epoch_19.ckpt\n",
      "Epoch 19: 100%|██████████| 351/351 [03:57<00:00,  1.48it/s, v_num=0, train_loss=4.290, val_loss=3.790, val_ari=0.101, val_nmi=0.171, train_ari=0.092, train_nmi=0.152]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 351/351 [04:01<00:00,  1.45it/s, v_num=0, train_loss=4.290, val_loss=3.790, val_ari=0.101, val_nmi=0.171, train_ari=0.092, train_nmi=0.152]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M | train\n",
      "1 | projection_head | SimCLRProjectionHead | 328 K  | train\n",
      "2 | criterion       | NTXentLoss           | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "46.022    Total estimated model params size (MB)\n",
      "77        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 351/351 [06:49<00:00,  0.86it/s, v_num=0, train_loss=4.660, val_loss=3.880, val_ari=0.0582, val_nmi=0.108, train_ari=0.0394, train_nmi=0.0753]    Zapisano model po epoce 9: checkpoints/simclr_cifar10_pretrained_0.01\\model_epoch_9.ckpt\n",
      "Epoch 14: 100%|██████████| 351/351 [04:55<00:00,  1.19it/s, v_num=0, train_loss=4.610, val_loss=3.850, val_ari=0.0716, val_nmi=0.139, train_ari=0.0581, train_nmi=0.0999]Zapisano model po epoce 14: checkpoints/simclr_cifar10_pretrained_0.01\\model_epoch_14.ckpt\n",
      "Epoch 19: 100%|██████████| 351/351 [08:23<00:00,  0.70it/s, v_num=0, train_loss=4.490, val_loss=3.810, val_ari=0.0693, val_nmi=0.145, train_ari=0.0667, train_nmi=0.108] Zapisano model po epoce 19: checkpoints/simclr_cifar10_pretrained_0.01\\model_epoch_19.ckpt\n",
      "Epoch 19: 100%|██████████| 351/351 [08:24<00:00,  0.70it/s, v_num=0, train_loss=4.490, val_loss=3.810, val_ari=0.0693, val_nmi=0.145, train_ari=0.0667, train_nmi=0.108]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 351/351 [08:27<00:00,  0.69it/s, v_num=0, train_loss=4.490, val_loss=3.810, val_ari=0.0693, val_nmi=0.145, train_ari=0.0667, train_nmi=0.108]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M | train\n",
      "1 | projection_head | SimCLRProjectionHead | 328 K  | train\n",
      "2 | criterion       | NTXentLoss           | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "46.022    Total estimated model params size (MB)\n",
      "77        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  54%|█████▎    | 188/351 [11:22<09:51,  0.28it/s, v_num=0, train_loss=5.130, val_loss=4.160, val_ari=0.0626, val_nmi=0.107, train_ari=0.0306, train_nmi=0.0619]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:150\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:339\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    337\u001b[39m     \u001b[38;5;28mself\u001b[39m.batch_progress.is_last_batch = data_fetcher.done\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_callback_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_train_batch_end\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m call._call_lightning_module_hook(trainer, \u001b[33m\"\u001b[39m\u001b[33mon_train_batch_end\u001b[39m\u001b[33m\"\u001b[39m, batch_output, batch, batch_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:227\u001b[39m, in \u001b[36m_call_callback_hooks\u001b[39m\u001b[34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[39m\n\u001b[32m    226\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Callback]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback.state_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m             \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pl_module:\n\u001b[32m    230\u001b[39m     \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\progress\\tqdm_progress.py:279\u001b[39m, in \u001b[36mTQDMProgressBar.on_train_batch_end\u001b[39m\u001b[34m(self, trainer, pl_module, outputs, batch, batch_idx)\u001b[39m\n\u001b[32m    278\u001b[39m _update_n(\u001b[38;5;28mself\u001b[39m.train_progress_bar, n)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28mself\u001b[39m.train_progress_bar.set_postfix(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\progress\\progress_bar.py:198\u001b[39m, in \u001b[36mProgressBar.get_metrics\u001b[39m\u001b[34m(self, trainer, pl_module)\u001b[39m\n\u001b[32m    197\u001b[39m standard_metrics = get_standard_metrics(trainer)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m pbar_metrics = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprogress_bar_metrics\u001b[49m\n\u001b[32m    199\u001b[39m duplicates = \u001b[38;5;28mlist\u001b[39m(standard_metrics.keys() & pbar_metrics.keys())\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1667\u001b[39m, in \u001b[36mTrainer.progress_bar_metrics\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1661\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"The metrics sent to the progress bar.\u001b[39;00m\n\u001b[32m   1662\u001b[39m \n\u001b[32m   1663\u001b[39m \u001b[33;03mThis includes metrics logged via :meth:`~pytorch_lightning.core.LightningModule.log` with the\u001b[39;00m\n\u001b[32m   1664\u001b[39m \u001b[33;03m:paramref:`~pytorch_lightning.core.LightningModule.log.prog_bar` argument set.\u001b[39;00m\n\u001b[32m   1665\u001b[39m \n\u001b[32m   1666\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_logger_connector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprogress_bar_metrics\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:254\u001b[39m, in \u001b[36m_LoggerConnector.progress_bar_metrics\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer._results:\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetrics\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mpbar\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._progress_bar_metrics.update(metrics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:235\u001b[39m, in \u001b[36m_LoggerConnector.metrics\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer._results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_results\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mon_step\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\result.py:493\u001b[39m, in \u001b[36m_ResultCollection.metrics\u001b[39m\u001b[34m(self, on_step)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result_metric.meta.prog_bar:\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m         metrics[\u001b[33m\"\u001b[39m\u001b[33mpbar\u001b[39m\u001b[33m\"\u001b[39m][forked_name] = \u001b[43mconvert_tensors_to_scalars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\lightning_fabric\\utilities\\apply_func.py:136\u001b[39m, in \u001b[36mconvert_tensors_to_scalars\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m value.item()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_item\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\lightning_utilities\\core\\apply_func.py:66\u001b[39m, in \u001b[36mapply_to_collection\u001b[39m\u001b[34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype):  \u001b[38;5;66;03m# single element\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data.\u001b[34m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, dtype) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):  \u001b[38;5;66;03m# 1d homogeneous list\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\lightning_fabric\\utilities\\apply_func.py:134\u001b[39m, in \u001b[36mconvert_tensors_to_scalars.<locals>.to_item\u001b[39m\u001b[34m(value)\u001b[39m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    132\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe metric `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` does not contain a single element, thus it cannot be converted to a scalar.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    133\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     10\u001b[39m checkpoint_callback = SaveAtEpochsCallback(\n\u001b[32m     11\u001b[39m     save_epochs=[\u001b[32m10\u001b[39m, \u001b[32m15\u001b[39m, \u001b[32m20\u001b[39m],\n\u001b[32m     12\u001b[39m     dirpath=dirpath\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m trainer = pl.Trainer(\n\u001b[32m     16\u001b[39m     accelerator=\u001b[33m'\u001b[39m\u001b[33mgpu\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     17\u001b[39m     devices=\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     default_root_dir=dirpath,\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimclr_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_train_cifar10_simclr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_val_cifar10_simclr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "for lr, backbone_type in product(LEARNING_RATE, BACKBONE_TYPE):\n",
    "    simclr_model = SimCLRModel(\n",
    "        backbone_type=backbone_type + '_resnet18',\n",
    "        lr=lr,\n",
    "        max_epochs=NUM_EPOCHS,\n",
    "    )\n",
    "\n",
    "    dirpath = f\"checkpoints/simclr_cifar10_{backbone_type}_{lr}\"\n",
    "\n",
    "    checkpoint_callback = SaveAtEpochsCallback(\n",
    "        save_epochs=[10, 15, 20],\n",
    "        dirpath=dirpath\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='gpu',\n",
    "        devices=1,\n",
    "        max_epochs=NUM_EPOCHS,\n",
    "        log_every_n_steps=1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        default_root_dir=dirpath,\n",
    "    )\n",
    "\n",
    "    trainer.fit(simclr_model, dl_train_cifar10_simclr, dl_val_cifar10_simclr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b65e23",
   "metadata": {},
   "source": [
    "# CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64ee55e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train_cifar100_simclr = torch.utils.data.DataLoader(\n",
    "    train_ssl_cifar100_simclr,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "dl_val_cifar100_simclr = torch.utils.data.DataLoader(\n",
    "    test_cifar100_simclr,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0014f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "import os\n",
    "\n",
    "class SaveAtEpochsCallback(Callback):\n",
    "    def __init__(self, save_epochs, dirpath=\"checkpoints\"):\n",
    "        super().__init__()\n",
    "        self.save_epochs = set(save_epochs)\n",
    "        self.dirpath = dirpath\n",
    "        os.makedirs(self.dirpath, exist_ok=True)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        current_epoch = trainer.current_epoch  # epoka 0-based\n",
    "        if current_epoch + 1 in self.save_epochs:\n",
    "            filename = f\"model_epoch_{current_epoch}.ckpt\"\n",
    "            path = os.path.join(self.dirpath, filename)\n",
    "            trainer.save_checkpoint(path)\n",
    "            print(f\"Zapisano model po epoce {current_epoch}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfdd6917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M | train\n",
      "1 | projection_head | SimCLRProjectionHead | 328 K  | train\n",
      "2 | criterion       | NTXentLoss           | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "46.022    Total estimated model params size (MB)\n",
      "77        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 351/351 [03:37<00:00,  1.61it/s, v_num=0, train_loss=4.160, val_loss=3.780, val_ari=0.0156, val_nmi=0.137, train_ari=0.0134, train_nmi=0.115]  Zapisano model po epoce 9: checkpoints/simclr_cifar100_pretrained_0.001\\model_epoch_9.ckpt\n",
      "Epoch 14: 100%|██████████| 351/351 [03:36<00:00,  1.62it/s, v_num=0, train_loss=4.190, val_loss=3.730, val_ari=0.0175, val_nmi=0.151, train_ari=0.014, train_nmi=0.116] Zapisano model po epoce 14: checkpoints/simclr_cifar100_pretrained_0.001\\model_epoch_14.ckpt\n",
      "Epoch 19: 100%|██████████| 351/351 [03:35<00:00,  1.63it/s, v_num=0, train_loss=4.140, val_loss=3.730, val_ari=0.0176, val_nmi=0.152, train_ari=0.0149, train_nmi=0.126]Zapisano model po epoce 19: checkpoints/simclr_cifar100_pretrained_0.001\\model_epoch_19.ckpt\n",
      "Epoch 19: 100%|██████████| 351/351 [03:36<00:00,  1.62it/s, v_num=0, train_loss=4.140, val_loss=3.730, val_ari=0.0176, val_nmi=0.152, train_ari=0.0149, train_nmi=0.126]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 351/351 [03:40<00:00,  1.59it/s, v_num=0, train_loss=4.140, val_loss=3.730, val_ari=0.0176, val_nmi=0.152, train_ari=0.0149, train_nmi=0.126]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M | train\n",
      "1 | projection_head | SimCLRProjectionHead | 328 K  | train\n",
      "2 | criterion       | NTXentLoss           | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "46.022    Total estimated model params size (MB)\n",
      "77        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 351/351 [03:41<00:00,  1.59it/s, v_num=0, train_loss=4.450, val_loss=3.830, val_ari=0.0123, val_nmi=0.111, train_ari=0.00948, train_nmi=0.083]   Zapisano model po epoce 9: checkpoints/simclr_cifar100_random_0.001\\model_epoch_9.ckpt\n",
      "Epoch 14: 100%|██████████| 351/351 [03:37<00:00,  1.62it/s, v_num=0, train_loss=4.410, val_loss=3.780, val_ari=0.013, val_nmi=0.119, train_ari=0.0104, train_nmi=0.0911]  Zapisano model po epoce 14: checkpoints/simclr_cifar100_random_0.001\\model_epoch_14.ckpt\n",
      "Epoch 19: 100%|██████████| 351/351 [03:36<00:00,  1.62it/s, v_num=0, train_loss=4.250, val_loss=3.770, val_ari=0.0134, val_nmi=0.120, train_ari=0.0108, train_nmi=0.0951]Zapisano model po epoce 19: checkpoints/simclr_cifar100_random_0.001\\model_epoch_19.ckpt\n",
      "Epoch 19: 100%|██████████| 351/351 [03:36<00:00,  1.62it/s, v_num=0, train_loss=4.250, val_loss=3.770, val_ari=0.0134, val_nmi=0.120, train_ari=0.0108, train_nmi=0.0951]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 351/351 [03:39<00:00,  1.60it/s, v_num=0, train_loss=4.250, val_loss=3.770, val_ari=0.0134, val_nmi=0.120, train_ari=0.0108, train_nmi=0.0951]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M | train\n",
      "1 | projection_head | SimCLRProjectionHead | 328 K  | train\n",
      "2 | criterion       | NTXentLoss           | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "46.022    Total estimated model params size (MB)\n",
      "77        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 351/351 [03:35<00:00,  1.63it/s, v_num=0, train_loss=4.510, val_loss=3.890, val_ari=0.00741, val_nmi=0.0733, train_ari=0.00551, train_nmi=0.0471]Zapisano model po epoce 9: checkpoints/simclr_cifar100_pretrained_0.01\\model_epoch_9.ckpt\n",
      "Epoch 14: 100%|██████████| 351/351 [03:37<00:00,  1.61it/s, v_num=0, train_loss=4.410, val_loss=3.800, val_ari=0.00807, val_nmi=0.0808, train_ari=0.00719, train_nmi=0.0609]Zapisano model po epoce 14: checkpoints/simclr_cifar100_pretrained_0.01\\model_epoch_14.ckpt\n",
      "Epoch 19: 100%|██████████| 351/351 [03:36<00:00,  1.62it/s, v_num=0, train_loss=4.330, val_loss=3.790, val_ari=0.00955, val_nmi=0.0884, train_ari=0.00823, train_nmi=0.0717]Zapisano model po epoce 19: checkpoints/simclr_cifar100_pretrained_0.01\\model_epoch_19.ckpt\n",
      "Epoch 19: 100%|██████████| 351/351 [03:37<00:00,  1.62it/s, v_num=0, train_loss=4.330, val_loss=3.790, val_ari=0.00955, val_nmi=0.0884, train_ari=0.00823, train_nmi=0.0717]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 351/351 [03:41<00:00,  1.59it/s, v_num=0, train_loss=4.330, val_loss=3.790, val_ari=0.00955, val_nmi=0.0884, train_ari=0.00823, train_nmi=0.0717]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M | train\n",
      "1 | projection_head | SimCLRProjectionHead | 328 K  | train\n",
      "2 | criterion       | NTXentLoss           | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "46.022    Total estimated model params size (MB)\n",
      "77        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 351/351 [03:34<00:00,  1.64it/s, v_num=0, train_loss=4.470, val_loss=3.860, val_ari=0.00913, val_nmi=0.0953, train_ari=0.00652, train_nmi=0.0608]Zapisano model po epoce 9: checkpoints/simclr_cifar100_random_0.01\\model_epoch_9.ckpt\n",
      "Epoch 14: 100%|██████████| 351/351 [03:35<00:00,  1.63it/s, v_num=0, train_loss=4.400, val_loss=3.800, val_ari=0.00931, val_nmi=0.0959, train_ari=0.0083, train_nmi=0.0754] Zapisano model po epoce 14: checkpoints/simclr_cifar100_random_0.01\\model_epoch_14.ckpt\n",
      "Epoch 19: 100%|██████████| 351/351 [03:36<00:00,  1.62it/s, v_num=0, train_loss=4.260, val_loss=3.790, val_ari=0.00948, val_nmi=0.0948, train_ari=0.00923, train_nmi=0.0806]Zapisano model po epoce 19: checkpoints/simclr_cifar100_random_0.01\\model_epoch_19.ckpt\n",
      "Epoch 19: 100%|██████████| 351/351 [03:36<00:00,  1.62it/s, v_num=0, train_loss=4.260, val_loss=3.790, val_ari=0.00948, val_nmi=0.0948, train_ari=0.00923, train_nmi=0.0806]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 351/351 [03:40<00:00,  1.59it/s, v_num=0, train_loss=4.260, val_loss=3.790, val_ari=0.00948, val_nmi=0.0948, train_ari=0.00923, train_nmi=0.0806]\n"
     ]
    }
   ],
   "source": [
    "for lr, backbone_type in product(LEARNING_RATE, BACKBONE_TYPE):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    simclr_model = SimCLRModel(\n",
    "        backbone_type=backbone_type + '_resnet18',\n",
    "        lr=lr,\n",
    "        max_epochs=NUM_EPOCHS,\n",
    "    )\n",
    "\n",
    "    dirpath = f\"checkpoints/simclr_cifar100_{backbone_type}_{lr}\"\n",
    "\n",
    "    checkpoint_callback = SaveAtEpochsCallback(\n",
    "        save_epochs=[10, 15, 20],\n",
    "        dirpath=dirpath\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='gpu',\n",
    "        devices=1,\n",
    "        max_epochs=NUM_EPOCHS,\n",
    "        log_every_n_steps=1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        default_root_dir=dirpath,\n",
    "    )\n",
    "\n",
    "    trainer.fit(simclr_model, dl_train_cifar100_simclr, dl_val_cifar100_simclr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03758712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
