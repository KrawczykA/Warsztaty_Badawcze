{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-26T17:11:19.837427Z",
     "start_time": "2025-05-26T17:10:50.435751Z"
    }
   },
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import xml.etree.ElementTree as ET\n",
    "import shutil\n",
    "\n",
    "imagenet_train_dir = \"data/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/train\"\n",
    "imagenet_val_dir = \"data/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/val\"\n",
    "imagenet_val_restructured_dir = \"data/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/val_restructured\"\n",
    "annotations_dir = \"data/imagenet-object-localization-challenge/ILSVRC/Annotations/CLS-LOC/val\"\n",
    "\n",
    "def restructure_val_dir():\n",
    "    if os.path.exists(imagenet_val_restructured_dir):\n",
    "        print(\"Restructured validation directory already exists.\")\n",
    "        return imagenet_val_restructured_dir\n",
    "\n",
    "    print(\"Restructuring validation directory...\")\n",
    "\n",
    "    # Get the class mapping from training directory\n",
    "    classes = [d.name for d in os.scandir(imagenet_train_dir) if d.is_dir()]\n",
    "\n",
    "    # Create class directories in the restructured validation directory\n",
    "    for class_name in classes:\n",
    "        os.makedirs(os.path.join(imagenet_val_restructured_dir, class_name), exist_ok=True)\n",
    "\n",
    "    # Function to extract class ID from XML annotation file\n",
    "    def get_class_from_annotation(xml_path):\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        # Get the first object's name (class)\n",
    "        obj = root.find('object')\n",
    "        if obj is not None:\n",
    "            return obj.find('name').text\n",
    "        return None\n",
    "\n",
    "    # Process each validation image\n",
    "    for img_name in os.listdir(imagenet_val_dir):\n",
    "        if not img_name.endswith('.JPEG'):\n",
    "            continue\n",
    "\n",
    "        # Get base name without extension\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "\n",
    "        # Find corresponding annotation file\n",
    "        xml_path = os.path.join(annotations_dir, base_name + '.xml')\n",
    "\n",
    "        if os.path.exists(xml_path):\n",
    "            class_name = get_class_from_annotation(xml_path)\n",
    "            if class_name and class_name in classes:\n",
    "                # Copy image to the appropriate class directory\n",
    "                src_path = os.path.join(imagenet_val_dir, img_name)\n",
    "                dst_path = os.path.join(imagenet_val_restructured_dir, class_name, img_name)\n",
    "                shutil.copy(src_path, dst_path)\n",
    "\n",
    "    print(\"Validation directory restructured successfully.\")\n",
    "    return imagenet_val_restructured_dir\n",
    "\n",
    "# Użycie biblioteki Lightly do transformacji i komponentów SSL\n",
    "import lightly\n",
    "from lightly.transforms import SimCLRTransform, DINOTransform, MAETransform\n",
    "\n",
    "\n",
    "# Wykrycie urządzenia do trenowania (GPU jeśli dostępne)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# --- Przygotowanie zbiorów danych CIFAR10 i CIFAR100 ---\n",
    "# Transformacje dla trenowania nadzorowanego (baseline i linear probe):\n",
    "# losowe przycięcie i odbicie (augmentacja) + normalizacja.\n",
    "train_transform_supervised = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),  # średnie i std dla CIFAR\n",
    "                         std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Transformacja dla zbioru testowego (tylko skalowanie do tensoru i normalizacja).\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                         std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Transformacje dla metod samonadzorowanych:\n",
    "# - Dla SimCLR/MoCo/BYOL: dwie zaugmentowane wersje obrazu.\n",
    "transform_simclr = SimCLRTransform(input_size=32)   # input_size=32 dla CIFAR\n",
    "# - Dla DINO: transformacja generująca 2 widoki globalne i 6 lokalnych (domyślnie).\n",
    "transform_dino = DINOTransform(global_crop_size=32, local_crop_size=16,  # dopasowanie do mniejszych obrazków\n",
    "                               global_crop_scale=(0.5, 1.0), local_crop_scale=(0.2, 0.5))\n",
    "# - Dla MAE/SimMIM: jedna widok z losowymi augmentacjami (proste augmentacje).\n",
    "transform_mae = MAETransform()\n",
    "\n",
    "# Ładowanie danych CIFAR100 (train i test)\n",
    "train_dataset_cifar100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True,\n",
    "                                                       transform=None)  # transform ustawimy później per metoda\n",
    "test_dataset_cifar100 = torchvision.datasets.CIFAR100(root='./data', train=False, download=True,\n",
    "                                                      transform=test_transform)\n",
    "# Ładowanie danych CIFAR10 (train i test)\n",
    "train_dataset_cifar10 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,\n",
    "                                                     transform=None)\n",
    "test_dataset_cifar10 = torchvision.datasets.CIFAR10(root='./data', train=False, download=True,\n",
    "                                                    transform=test_transform)\n",
    "\n",
    "# Dataloader dla ewaluacji (testy) – tutaj wykorzystamy go do obliczania cech i ewaluacji\n",
    "test_loader_cifar100 = torch.utils.data.DataLoader(test_dataset_cifar100, batch_size=256, shuffle=False)\n",
    "test_loader_cifar10 = torch.utils.data.DataLoader(test_dataset_cifar10, batch_size=256, shuffle=False)\n",
    "\n",
    "# (Opcjonalnie) Przygotowanie zbioru ImageNet-1k, jeżeli dostępny na dysku:\n",
    "imagenet_train_dir = 'data/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/train'  # <-- Uwaga: ustawić poprawną ścieżkę jeśli dane dostępne\n",
    "imagenet_val_dir = 'data/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/val'\n",
    "imagenet_train_dataset = None\n",
    "imagenet_val_dataset = None\n",
    "if os.path.exists(imagenet_train_dir):\n",
    "    # Transformacje dla ImageNet: wymiary 224x224 jak w standardowych modelach\n",
    "    transform_simclr_imagenet = SimCLRTransform(input_size=224)\n",
    "    transform_dino_imagenet = DINOTransform()  # domyślne parametry dla DINO (224 global, 96 lokal)\n",
    "    transform_mae_imagenet = MAETransform()\n",
    "    # transformacje dla baseline i linear eval na ImageNet (przycięcie centralne dla val)\n",
    "    train_transform_supervised_imnet = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.2, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                             std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    val_transform_imnet = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                             std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    imagenet_val_dir_to_use = restructure_val_dir()\n",
    "\n",
    "    # Używamy ImageFolder do wczytania danych z katalogu\n",
    "    imagenet_train_dataset = torchvision.datasets.ImageFolder(root=imagenet_train_dir,\n",
    "                                                              transform=None)  # transform ustawimy dynamicznie\n",
    "    imagenet_val_dataset = torchvision.datasets.ImageFolder(root=imagenet_val_dir_to_use,\n",
    "                                                            transform=val_transform_imnet)\n",
    "    print(\"ImageNet datasets prepared.\")\n",
    "else:\n",
    "    print(\"ImageNet data not found, skipping ImageNet training in this run.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hussein/pytoniec/lib/python3.13/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restructuring validation directory...\n",
      "Validation directory restructured successfully.\n",
      "ImageNet datasets prepared.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-26T17:13:44.957725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from lightly.loss import NTXentLoss, DINOLoss\n",
    "from lightly.models.modules.heads import SimCLRProjectionHead, DINOProjectionHead, BYOLProjectionHead, BYOLPredictionHead\n",
    "from lightly.models.utils import update_momentum, deactivate_requires_grad\n",
    "\n",
    "# 2.1 Trenowanie masked autoencoder (MAE/SimMIM) na zbiorze nieoznaczonym\n",
    "def pretrain_masked_autoencoder(train_dataset, epochs=20, batch_size=128, lr=1.5e-4):\n",
    "    \"\"\"\n",
    "    Trenuje model typu Masked Autoencoder na podanym zbiorze danych.\n",
    "    Zwraca wytrenowany encoder (backbone) oraz cały model (encoder+decoder).\n",
    "    \"\"\"\n",
    "    # Ustawiamy transformację dla datasetu (MAETransform przygotowuje random crop i normalizację)\n",
    "    train_dataset.transform = transform_mae\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # Tworzymy model - ViT jako encoder, prosta warstwa liniowa jako decoder (SimMIM styl)\n",
    "    vit = torchvision.models.vit_b_16(pretrained=False)  # ViT-base patch16; dla CIFAR może być nadmiarowy\n",
    "    # Dostosowanie: zmieniamy rozmiar wejścia patch (CIFAR obraz 32x32, patch 16 -> 2x2 patchy, to za mało)\n",
    "    # Alternatywnie: powiększamy obrazy CIFAR do 224 wewnątrz transformacji by użyć ViT patch16.\n",
    "    # (Tutaj zakładamy, że transformacja MAETransform może wewnętrznie robić resize do 224; jeśli nie, warto dodać Resize(224) do transformacji dla CIFAR.)\n",
    "    # Budujemy model maskowanego autoenkodera:\n",
    "    class MaskedAutoencoder(nn.Module):\n",
    "        def __init__(self, vit_encoder):\n",
    "            super().__init__()\n",
    "            self.encoder = lightly.models.modules.masked_vision_transformer_torchvision.MaskedVisionTransformerTorchvision(vit=vit_encoder)\n",
    "            # Decoder: pojedyncza warstwa liniowa mapująca latent do rozmiaru patch (patch_size^2 * 3 kanały)\n",
    "            hidden_dim = vit_encoder.hidden_dim if hasattr(vit_encoder, 'hidden_dim') else vit_encoder.heads.head.in_features\n",
    "            patch_size = vit_encoder.patch_size if hasattr(vit_encoder, 'patch_size') else 16\n",
    "            self.decoder = nn.Linear(hidden_dim, patch_size**2 * 3)\n",
    "        def forward(self, images):\n",
    "            # Losowe maskowanie tokenów (75% tokenów maskowane)\n",
    "            batch_size = images.shape[0]\n",
    "            seq_len = self.encoder.seq_length  # długość sekwencji patchy (łącznie z tokenem cls jeśli jest)\n",
    "            # Generowanie maski losowej\n",
    "            idx_keep, idx_mask = lightly.models.utils.random_token_mask((batch_size, seq_len), mask_ratio=0.75, device=images.device)\n",
    "            # Encoder dostaje pełny obraz oraz indeksy maskowanych tokenów\n",
    "            encoded = self.encoder.encode(images=images, idx_mask=idx_mask)\n",
    "            # Wybieramy tylko reprezentacje niezamaskowanych tokenów (MAE tak robi przed dekoderem)\n",
    "            encoded_masked = lightly.models.utils.get_at_index(encoded, idx_mask)\n",
    "            # Dekoder: próba rekonstrukcji oryginalnych patchy dla maskowanych pozycji\n",
    "            preds = self.decoder(encoded_masked)  # wyniki dla maskowanych patchy\n",
    "            # Wyznaczamy \"ground truth\" - faktyczne piksele maskowanych patchy\n",
    "            patches = lightly.models.utils.patchify(images, patch_size)\n",
    "            # Uwaga: jeśli ViT dodaje token klas (cls token) to indeksy patchy trzeba zmodyfikować (idx_mask-1)\n",
    "            target = lightly.models.utils.get_at_index(patches, idx_mask - 1)\n",
    "            return preds, target\n",
    "\n",
    "    # Inicjalizacja modelu i ustawienie na urządzenie\n",
    "    mae_model = MaskedAutoencoder(vit).to(device)\n",
    "    # Funkcja kosztu - błąd L1 (średni bezwzględny) pomiędzy patchami\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.AdamW(mae_model.parameters(), lr=lr)\n",
    "    mae_model.train()\n",
    "    print(\">>> Trenowanie Masked Autoencoder przez {} epok...\".format(epochs))\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for (images, _) in train_loader:\n",
    "            images = images.to(device)\n",
    "            preds, targets = mae_model(images)        # forward prze maskowany autoenkoder\n",
    "            loss = criterion(preds, targets)          # oblicz stratę rekonstrukcji\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"[MAE] Epoka {epoch+1}/{epochs}, średni L1 loss: {avg_loss:.4f}\")\n",
    "    # Zwracamy wytrenowany encoder (vit) oraz cały model (encoder+decoder)\n",
    "    return mae_model.encoder.backbone, mae_model\n",
    "\n",
    "# 2.2 Trenowanie metody SimCLR (kontrastywna) na zbiorze nieoznaczonym\n",
    "def pretrain_simclr(train_dataset, epochs=20, batch_size=128, lr=6e-2):\n",
    "    \"\"\"\n",
    "    Trenuje model SimCLR (ResNet18 + projection head) na podanym zbiorze danych.\n",
    "    Zwraca wytrenowany backbone (ResNet bez klasyfikatora).\n",
    "    \"\"\"\n",
    "    # Ustawienie transformacji dwóch widoków na dataset\n",
    "    train_dataset.transform = transform_simclr\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # Tworzymy backbone (ResNet18) i usuwamy ostatnią warstwę klasyfikacji\n",
    "    resnet = torchvision.models.resnet18(pretrained=False)\n",
    "    backbone = nn.Sequential(*list(resnet.children())[:-1])  # do przedostatniej warstwy (global avg pool)\n",
    "    backbone_output_dim = resnet.fc.in_features  # wymiar cech wyjściowych backbone (512 dla ResNet18)\n",
    "    # Projekcyjna głowica SimCLR: MLP (hidden_dim -> output_dim=128 zazwyczaj)\n",
    "    projection_head = SimCLRProjectionHead(input_dim=backbone_output_dim, hidden_dim=backbone_output_dim, output_dim=128)\n",
    "    # Funkcja kosztu NT-Xent (InfoNCE) – Lightly ma implementację. Bez memory bank (tu SimCLR, używamy tylko batch negatywów).\n",
    "    criterion = NTXentLoss()  # domyślnie temperature=0.5\n",
    "    optimizer = torch.optim.SGD(list(backbone.parameters()) + list(projection_head.parameters()),\n",
    "                                lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    backbone.to(device)\n",
    "    projection_head.to(device)\n",
    "    backbone.train()\n",
    "    projection_head.train()\n",
    "    print(\">>> Trenowanie SimCLR przez {} epok...\".format(epochs))\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for (views, _) in train_loader:  # views to tupla (x_i, x_j) augmentacji\n",
    "            x1, x2 = views[0].to(device), views[1].to(device)\n",
    "            # Obliczamy reprezentacje h dla obu widoków\n",
    "            h1 = backbone(x1).flatten(start_dim=1)\n",
    "            h2 = backbone(x2).flatten(start_dim=1)\n",
    "            # Projekcja z MLP (z) - wektor 128-d do kontrastowania\n",
    "            z1 = projection_head(h1)\n",
    "            z2 = projection_head(h2)\n",
    "            # Oblicz strata NTXentLoss (przyjmuje 2 tensory: pozytywne pary)\n",
    "            loss = criterion(z1, z2)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"[SimCLR] Epoka {epoch+1}/{epochs}, średni loss: {avg_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "    return backbone\n",
    "\n",
    "# 2.2 Trenowanie metody MoCo (Momentum Contrast) na zbiorze nieoznaczonym\n",
    "def pretrain_moco(train_dataset, epochs=20, batch_size=128, lr=0.06, memory_bank_size=4096):\n",
    "    \"\"\"\n",
    "    Trenuje model MoCo v2 (ResNet18 z encoderem kluczy i kolejką) na podanym zbiorze danych.\n",
    "    Zwraca wytrenowany backbone (encoder zapytań).\n",
    "    \"\"\"\n",
    "    # Ustawienie transformacji dwóch widoków (tak jak SimCLR)\n",
    "    train_dataset.transform = transform_simclr\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # Backbone główny (query encoder) i momentum backbone (key encoder)\n",
    "    resnet = torchvision.models.resnet18(pretrained=False)\n",
    "    query_encoder = nn.Sequential(*list(resnet.children())[:-1])\n",
    "    backbone_output_dim = resnet.fc.in_features  # 512\n",
    "    # Kopiujemy encoder do key_encoder i zamrażamy gradienty w nim\n",
    "    key_encoder = copy.deepcopy(query_encoder)\n",
    "    deactivate_requires_grad(key_encoder)  # wyłączenie obliczania grad dla momentum encodera\n",
    "    # Projekcyjne głowice dla obu encoderów (MoCo v2 używa MLP jak SimCLR)\n",
    "    proj_q = SimCLRProjectionHead(input_dim=backbone_output_dim, hidden_dim=backbone_output_dim, output_dim=128)\n",
    "    proj_k = copy.deepcopy(proj_q)\n",
    "    deactivate_requires_grad(proj_k)\n",
    "    # Bufor (queue) na negatywne przykłady\n",
    "    # Inicjalizacja kolejki losowymi wektorami jednostkowymi\n",
    "    queue_size = memory_bank_size\n",
    "    feature_dim = 128\n",
    "    queue = F.normalize(torch.randn(queue_size, feature_dim, device=device), dim=1)\n",
    "    queue_ptr = 0  # wskaźnik pozycji do nadpisania w kolejce\n",
    "\n",
    "    # Funkcja strat InfoNCE (NTXentLoss) z memory bank, żeby wykorzystywać kolejkę jako negatywy\n",
    "    criterion = NTXentLoss(memory_bank_size=(memory_bank_size, feature_dim))\n",
    "    optimizer = torch.optim.SGD(list(query_encoder.parameters()) + list(proj_q.parameters()), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    query_encoder.to(device)\n",
    "    proj_q.to(device)\n",
    "    key_encoder.to(device)\n",
    "    proj_k.to(device)\n",
    "    query_encoder.train(); proj_q.train()\n",
    "    key_encoder.eval(); proj_k.eval()  # momentum encoder nie trenujemy (ewaluacja - nie aktualizuje BN)\n",
    "    print(\">>> Trenowanie MoCo przez {} epok...\".format(epochs))\n",
    "    momentum = 0.999  # współczynnik momentum do uaktualniania kluczowego encodera\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for (views, _) in train_loader:\n",
    "            x_q, x_k = views[0].to(device), views[1].to(device)  # pierwsza augmentacja jako query, druga jako key\n",
    "            # Forward przez query encoder\n",
    "            q_features = query_encoder(x_q).flatten(start_dim=1)\n",
    "            q_proj = F.normalize(proj_q(q_features), dim=1)  # znormalizowane z_q (128-dim)\n",
    "            # Forward przez key encoder (bez gradientu)\n",
    "            with torch.no_grad():\n",
    "                # Aktualizujemy parametry key_encoder z momentum (z każdą iteracją upodabniamy do query_encoder)\n",
    "                update_momentum(query_encoder, key_encoder, m=momentum)\n",
    "                update_momentum(proj_q, proj_k, m=momentum)\n",
    "                k_features = key_encoder(x_k).flatten(start_dim=1)\n",
    "                k_proj = F.normalize(proj_k(k_features), dim=1)  # z_k (target)\n",
    "            # Obliczamy podobieństwa z query do: pozytyw (k_proj tej samej próbki) oraz negatywy (wszystkie z kolejki)\n",
    "            # Lightly NTXentLoss z memory bank pozwala to uprościć: przekażemy q_proj i k_proj, gdzie k_proj zostanie dodany do memory bank automatycznie.\n",
    "            loss = criterion(q_proj, k_proj)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            # Uaktualnienie kolejki: dodajemy nowe k_proj do queue, zastępując najstarsze\n",
    "            batch_size_effective = k_proj.shape[0]\n",
    "            if batch_size_effective <= queue_size:\n",
    "                # nadpisz najstarsze elementy\n",
    "                queue[queue_ptr:queue_ptr+batch_size_effective, :] = k_proj.detach()\n",
    "                queue_ptr = (queue_ptr + batch_size_effective) % queue_size\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"[MoCo] Epoka {epoch+1}/{epochs}, średni loss: {avg_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "    return query_encoder  # zwracamy tylko encoder zapytań (wytrenowany backbone)\n",
    "\n",
    "# 2.3 Trenowanie metody BYOL (Bootstrap Your Own Latent) na zbiorze nieoznaczonym\n",
    "def pretrain_byol(train_dataset, epochs=20, batch_size=128, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Trenuje model BYOL (ResNet18 online + target) na podanym zbiorze danych.\n",
    "    Zwraca wytrenowany backbone (online network).\n",
    "    \"\"\"\n",
    "    # Ustawienie transformacji dwóch widoków (BYOL używa podobnych augmentacji jak SimCLR, ewentualnie dodając solarization, tutaj korzystamy z SimCLRTransform)\n",
    "    train_dataset.transform = transform_simclr\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # Definiujemy backbone sieci online i tworzymy kopię do sieci target\n",
    "    resnet = torchvision.models.resnet18(pretrained=False)\n",
    "    online_backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "    backbone_output_dim = resnet.fc.in_features  # 512\n",
    "    target_backbone = copy.deepcopy(online_backbone)\n",
    "    deactivate_requires_grad(target_backbone)  # sieć target nie ma gradientów\n",
    "    # Projekcja (MLP) i predykcja dla sieci online, projekcja dla sieci target\n",
    "    online_proj = BYOLProjectionHead(input_dim=backbone_output_dim, hidden_dim=backbone_output_dim, output_dim=256)\n",
    "    online_pred = BYOLPredictionHead(input_dim=256, hidden_dim=256, output_dim=256)\n",
    "    target_proj = copy.deepcopy(online_proj)\n",
    "    deactivate_requires_grad(target_proj)\n",
    "    # Optymalizujemy tylko parametry online (backbone, proj, pred)\n",
    "    optimizer = torch.optim.Adam(list(online_backbone.parameters()) + list(online_proj.parameters()) + list(online_pred.parameters()), lr=lr)\n",
    "    online_backbone.to(device); online_proj.to(device); online_pred.to(device)\n",
    "    target_backbone.to(device); target_proj.to(device)\n",
    "    online_backbone.train(); online_proj.train(); online_pred.train()\n",
    "    target_backbone.eval(); target_proj.eval()\n",
    "    momentum = 0.996  # współczynnik momentum do uaktualniania target network\n",
    "    print(\">>> Trenowanie BYOL przez {} epok...\".format(epochs))\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for (views, _) in train_loader:\n",
    "            x_a, x_b = views[0].to(device), views[1].to(device)  # dwie augmentacje\n",
    "            # Forward przez online network dla obu widoków\n",
    "            feat_a = online_backbone(x_a).flatten(start_dim=1)\n",
    "            feat_b = online_backbone(x_b).flatten(start_dim=1)\n",
    "            proj_a = online_proj(feat_a)\n",
    "            proj_b = online_proj(feat_b)\n",
    "            pred_a = online_pred(proj_a)  # predykcja dla a\n",
    "            pred_b = online_pred(proj_b)  # predykcja dla b\n",
    "            # Forward przez target network (bez grad)\n",
    "            with torch.no_grad():\n",
    "                # momentum update target sieci\n",
    "                update_momentum(online_backbone, target_backbone, m=momentum)\n",
    "                update_momentum(online_proj, target_proj, m=momentum)\n",
    "                # (target_pred nie ma, bo target sieć kończy na projekcji)\n",
    "                target_feat_a = target_backbone(x_a).flatten(start_dim=1)\n",
    "                target_feat_b = target_backbone(x_b).flatten(start_dim=1)\n",
    "                target_proj_a = target_proj(target_feat_a)\n",
    "                target_proj_b = target_proj(target_feat_b)\n",
    "            # Normalizacja wektorów projekcji i predykcji\n",
    "            pred_a_norm = F.normalize(pred_a, dim=1)\n",
    "            pred_b_norm = F.normalize(pred_b, dim=1)\n",
    "            target_a_norm = F.normalize(target_proj_b.detach(), dim=1)  # UWAGA: pred_a porównujemy z target z drugiego widoku\n",
    "            target_b_norm = F.normalize(target_proj_a.detach(), dim=1)\n",
    "            # Obliczenie straty MSE pomiędzy znormalizowanymi predykcjami online a docelowymi reprezentacjami target\n",
    "            loss = 2 - 2 * (pred_a_norm * target_a_norm).sum(dim=1).mean() - 2 * (pred_b_norm * target_b_norm).sum(dim=1).mean()\n",
    "            # (powyższe to równoważnik: loss = MSE(pred_a_norm, target_a_norm) + MSE(pred_b_norm, target_b_norm))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"[BYOL] Epoka {epoch+1}/{epochs}, średni loss: {avg_loss:.4f}\")\n",
    "    return online_backbone\n",
    "\n",
    "# 2.3 Trenowanie metody DINO (Distillation with No Labels) na zbiorze nieoznaczonym\n",
    "def pretrain_dino(train_dataset, epochs=20, batch_size=128, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Trenuje model DINO (ResNet18 student + momentum teacher) na podanym zbiorze danych.\n",
    "    Zwraca wytrenowany backbone (student backbone).\n",
    "    \"\"\"\n",
    "    # Ustawienie transformacji DINO (wiele widoków) na dataset\n",
    "    train_dataset.transform = transform_dino\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # Definiujemy backbone (ResNet18) dla studenta i kopiujemy dla nauczyciela\n",
    "    resnet = torchvision.models.resnet18(pretrained=False)\n",
    "    student_backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "    backbone_output_dim = resnet.fc.in_features  # 512\n",
    "    teacher_backbone = copy.deepcopy(student_backbone)\n",
    "    deactivate_requires_grad(teacher_backbone)\n",
    "    # Głowice projekcyjne DINO dla studenta i nauczyciela.\n",
    "    # Używamy DINOProjectionHead: parametry (in_dim, hidden_dim, bottleneck_dim, out_dim, [opcje])\n",
    "    student_head = DINOProjectionHead(input_dim=backbone_output_dim, hidden_dim=512, bottleneck_dim=256, output_dim=2048, freeze_last_layer=1)\n",
    "    teacher_head = copy.deepcopy(student_head)\n",
    "    # teacher_head nie zamrażamy w całości, ale podczas optymalizacji nie będziemy go aktualizować (nie jest w optimizer)\n",
    "    deactivate_requires_grad(teacher_head)\n",
    "    # Funkcja kosztu DINO – porównuje wyjścia nauczyciela i studenta (zawiera mechanizm centrowania i temperatury)\n",
    "    criterion = DINOLoss(output_dim=2048, warmup_teacher_temp_epochs=5)\n",
    "    # Optimizer tylko dla sieci studenta (backbone + head)\n",
    "    optimizer = torch.optim.Adam(list(student_backbone.parameters()) + list(student_head.parameters()), lr=lr)\n",
    "    # Harmonogram zmiany współczynnika momentum (od nieco mniejszego do 1)\n",
    "    # Będziemy liniowo zwiększać momentum nauczyciela od 0.996 do 1.0 w trakcie epok\n",
    "    initial_momentum = 0.996\n",
    "    final_momentum = 1.0\n",
    "    student_backbone.to(device); student_head.to(device)\n",
    "    teacher_backbone.to(device); teacher_head.to(device)\n",
    "    student_backbone.train(); student_head.train()\n",
    "    teacher_backbone.eval(); teacher_head.eval()\n",
    "    print(\">>> Trenowanie DINO przez {} epok...\".format(epochs))\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        # Wyznacz wartość momentum dla bieżącej epoki (cosine schedule)\n",
    "        momentum_val = initial_momentum + (final_momentum - initial_momentum) * (epoch / (epochs - 1))\n",
    "        for batch in train_loader:\n",
    "            views = batch[0]  # lista widoków augmentowanych (list length = 2 + n_local_views, domyślnie 8)\n",
    "            # Uaktualnienie wag nauczyciela (backbone i head) - momentum update przed forward\n",
    "            update_momentum(student_backbone, teacher_backbone, m=momentum_val)\n",
    "            update_momentum(student_head, teacher_head, m=momentum_val)\n",
    "            # Przenieś wszystkie widoki na GPU\n",
    "            views = [v.to(device) for v in views]\n",
    "            # DINO: nauczyciel otrzymuje tylko 2 globalne widoki (zwykle 224x224), student wszystkie\n",
    "            # Zakładamy, że transform_dino generuje 2 pierwsze widoki jako \"globalne\"\n",
    "            global_views = views[:2]\n",
    "            # Obliczenia forward:\n",
    "            # - Teacher output dla globalnych widoków (zatrzymujemy gradient)\n",
    "            with torch.no_grad():\n",
    "                teacher_out = [teacher_head(teacher_backbone(v).flatten(start_dim=1)) for v in global_views]\n",
    "            # - Student output dla wszystkich widoków\n",
    "            student_out = [student_head(student_backbone(v).flatten(start_dim=1)) for v in views]\n",
    "            # Oblicz stratę DINO (porównuje rozkłady wyjściowe teacher vs student)\n",
    "            loss = criterion(teacher_out, student_out, epoch=epoch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # DINO zaleca zablokowanie gradientów ostatniej warstwy projekcyjnej studenta na wczesnych epokach (freeze_last_layer)\n",
    "            student_head.cancel_last_layer_gradients(current_epoch=epoch)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"[DINO] Epoka {epoch+1}/{epochs}, średni loss: {avg_loss:.4f}\")\n",
    "    return student_backbone\n",
    "\n",
    "# 2.4 Trenowanie modelu nadzorowanego (supervised) na CIFAR100 (baseline)\n",
    "def train_supervised_classifier(train_dataset, num_classes=100, epochs=20, batch_size=128, lr=0.1):\n",
    "    \"\"\"\n",
    "    Trenuje model klasyfikacyjny (ResNet18) w sposób nadzorowany na podanym zbiorze (z etykietami).\n",
    "    Zwraca wytrenowany model (backbone + klasyfikator).\n",
    "    \"\"\"\n",
    "    # Ustawiamy transformacje augmentacyjne dla treningu nadzorowanego\n",
    "    train_dataset.transform = train_transform_supervised\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # Tworzymy model ResNet18 z random inicjalizacją (num_classes wyjściowych)\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    model.train()\n",
    "    print(\">>> Trenowanie modelu nadzorowanego (ResNet18) przez {} epok...\".format(epochs))\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for (images, labels) in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            # Obliczanie dokładności bieżącej partii (dla monitorowania)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        acc = 100.0 * correct / total\n",
    "        print(f\"[Supervised] Epoka {epoch+1}/{epochs}, loss: {avg_loss:.4f}, accuracy: {acc:.2f}%\")\n",
    "        scheduler.step()\n",
    "    return model\n",
    "\n",
    "# ---- Wykonanie treningów dla poszczególnych metod na CIFAR100 ----\n",
    "print(\"\\n=== Rozpoczęcie treningów SSL na CIFAR100 ===\")\n",
    "# # Etap podstawowy:\n",
    "# backbone_mae_cifar100, mae_model = pretrain_masked_autoencoder(train_dataset_cifar100, epochs=10, batch_size=128)\n",
    "# backbone_simclr_cifar100 = pretrain_simclr(train_dataset_cifar100, epochs=10, batch_size=128)\n",
    "# # Etap pośredni (dodatkowo trening MoCo i supervised baseline na CIFAR100):\n",
    "# backbone_moco_cifar100 = pretrain_moco(train_dataset_cifar100, epochs=10, batch_size=128)\n",
    "# supervised_model_cifar100 = train_supervised_classifier(train_dataset_cifar100, num_classes=100, epochs=10, batch_size=128)\n",
    "# backbone_supervised_cifar100 = nn.Sequential(*list(supervised_model_cifar100.children())[:-1])  # ekstrakcja backbone z modelu nadzorowanego\n",
    "# # Etap zaawansowany (self-distillation metody BYOL i DINO na CIFAR100):\n",
    "backbone_byol_cifar100 = pretrain_byol(train_dataset_cifar100, epochs=10, batch_size=128)\n",
    "backbone_dino_cifar100 = pretrain_dino(train_dataset_cifar100, epochs=10, batch_size=128)\n",
    "print(\"=== Zakończono pretraining SSL na CIFAR100 ===\\n\")\n",
    "\n",
    "# (Opcjonalnie) Trenowanie na ImageNet-1k dla etapów pośredniego/zaawansowanego\n",
    "if imagenet_train_dataset is not None:\n",
    "    print(\"=== Rozpoczęcie treningów SSL na ImageNet-1k (skala demonstracyjna) ===\")\n",
    "    # Ustawiamy odpowiednie transformacje dla ImageNet i tworzymy DataLoader\n",
    "    imagenet_train_dataset.transform = transform_simclr_imagenet\n",
    "    imnet_loader = torch.utils.data.DataLoader(imagenet_train_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
    "    # Dla przykładu trenujemy SimCLR i BYOL na ImageNet kilka epok (w praktyce potrzeba znacznie więcej)\n",
    "    # backbone_simclr_imnet = pretrain_simclr(imagenet_train_dataset, epochs=2, batch_size=256, lr=0.1)\n",
    "    # backbone_moco_imnet = pretrain_moco(imagenet_train_dataset, epochs=2, batch_size=256, lr=0.1, memory_bank_size=65536)\n",
    "    backbone_byol_imnet = pretrain_byol(imagenet_train_dataset, epochs=2, batch_size=256, lr=1e-3)\n",
    "    backbone_dino_imnet = pretrain_dino(imagenet_train_dataset, epochs=2, batch_size=256, lr=1e-3)\n",
    "    # Dla ImageNet również można by przeprowadzić linear probing lub ewaluację na CIFAR, ale pomijamy dalsze szczegóły w tym kodzie demonstracyjnym.\n",
    "    print(\"=== Zakończono pretraining SSL na ImageNet-1k ===\\n\")"
   ],
   "id": "e1d5a6d3d26b4edf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Rozpoczęcie treningów SSL na CIFAR100 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hussein/pytoniec/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hussein/pytoniec/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Trenowanie BYOL przez 10 epok...\n",
      "[BYOL] Epoka 1/10, średni loss: -1.9407\n",
      "[BYOL] Epoka 2/10, średni loss: -1.9996\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T17:12:22.821515Z",
     "start_time": "2025-05-26T17:12:22.819360Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "23132e26ccdc7e98",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8a8207501294b85a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
