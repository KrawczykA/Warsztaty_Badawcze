{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-26T17:11:19.837427Z",
     "start_time": "2025-05-26T17:10:50.435751Z"
    }
   },
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import xml.etree.ElementTree as ET\n",
    "import shutil\n",
    "\n",
    "imagenet_train_dir = \"data/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/train\"\n",
    "imagenet_val_dir = \"data/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/val\"\n",
    "imagenet_val_restructured_dir = \"data/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/val_restructured\"\n",
    "annotations_dir = \"data/imagenet-object-localization-challenge/ILSVRC/Annotations/CLS-LOC/val\"\n",
    "\n",
    "def restructure_val_dir():\n",
    "    if os.path.exists(imagenet_val_restructured_dir):\n",
    "        print(\"Restructured validation directory already exists.\")\n",
    "        return imagenet_val_restructured_dir\n",
    "\n",
    "    print(\"Restructuring validation directory...\")\n",
    "\n",
    "    # Get the class mapping from training directory\n",
    "    classes = [d.name for d in os.scandir(imagenet_train_dir) if d.is_dir()]\n",
    "\n",
    "    # Create class directories in the restructured validation directory\n",
    "    for class_name in classes:\n",
    "        os.makedirs(os.path.join(imagenet_val_restructured_dir, class_name), exist_ok=True)\n",
    "\n",
    "    # Function to extract class ID from XML annotation file\n",
    "    def get_class_from_annotation(xml_path):\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        # Get the first object's name (class)\n",
    "        obj = root.find('object')\n",
    "        if obj is not None:\n",
    "            return obj.find('name').text\n",
    "        return None\n",
    "\n",
    "    # Process each validation image\n",
    "    for img_name in os.listdir(imagenet_val_dir):\n",
    "        if not img_name.endswith('.JPEG'):\n",
    "            continue\n",
    "\n",
    "        # Get base name without extension\n",
    "        base_name = os.path.splitext(img_name)[0]\n",
    "\n",
    "        # Find corresponding annotation file\n",
    "        xml_path = os.path.join(annotations_dir, base_name + '.xml')\n",
    "\n",
    "        if os.path.exists(xml_path):\n",
    "            class_name = get_class_from_annotation(xml_path)\n",
    "            if class_name and class_name in classes:\n",
    "                # Copy image to the appropriate class directory\n",
    "                src_path = os.path.join(imagenet_val_dir, img_name)\n",
    "                dst_path = os.path.join(imagenet_val_restructured_dir, class_name, img_name)\n",
    "                shutil.copy(src_path, dst_path)\n",
    "\n",
    "    print(\"Validation directory restructured successfully.\")\n",
    "    return imagenet_val_restructured_dir\n",
    "\n",
    "# Użycie biblioteki Lightly do transformacji i komponentów SSL\n",
    "import lightly\n",
    "from lightly.transforms import SimCLRTransform, DINOTransform, MAETransform\n",
    "\n",
    "\n",
    "# Wykrycie urządzenia do trenowania (GPU jeśli dostępne)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# --- Przygotowanie zbiorów danych CIFAR10 i CIFAR100 ---\n",
    "# Transformacje dla trenowania nadzorowanego (baseline i linear probe):\n",
    "# losowe przycięcie i odbicie (augmentacja) + normalizacja.\n",
    "train_transform_supervised = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),  # średnie i std dla CIFAR\n",
    "                         std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Transformacja dla zbioru testowego (tylko skalowanie do tensoru i normalizacja).\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                         std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Transformacje dla metod samonadzorowanych:\n",
    "# - Dla SimCLR/MoCo/BYOL: dwie zaugmentowane wersje obrazu.\n",
    "transform_simclr = SimCLRTransform(input_size=32)   # input_size=32 dla CIFAR\n",
    "# - Dla DINO: transformacja generująca 2 widoki globalne i 6 lokalnych (domyślnie).\n",
    "transform_dino = DINOTransform(global_crop_size=32, local_crop_size=16,  # dopasowanie do mniejszych obrazków\n",
    "                               global_crop_scale=(0.5, 1.0), local_crop_scale=(0.2, 0.5))\n",
    "# - Dla MAE/SimMIM: jedna widok z losowymi augmentacjami (proste augmentacje).\n",
    "transform_mae = MAETransform()\n",
    "\n",
    "# Ładowanie danych CIFAR100 (train i test)\n",
    "train_dataset_cifar100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True,\n",
    "                                                       transform=None)  # transform ustawimy później per metoda\n",
    "test_dataset_cifar100 = torchvision.datasets.CIFAR100(root='./data', train=False, download=True,\n",
    "                                                      transform=test_transform)\n",
    "# Ładowanie danych CIFAR10 (train i test)\n",
    "train_dataset_cifar10 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,\n",
    "                                                     transform=None)\n",
    "test_dataset_cifar10 = torchvision.datasets.CIFAR10(root='./data', train=False, download=True,\n",
    "                                                    transform=test_transform)\n",
    "\n",
    "# Dataloader dla ewaluacji (testy) – tutaj wykorzystamy go do obliczania cech i ewaluacji\n",
    "test_loader_cifar100 = torch.utils.data.DataLoader(test_dataset_cifar100, batch_size=256, shuffle=False)\n",
    "test_loader_cifar10 = torch.utils.data.DataLoader(test_dataset_cifar10, batch_size=256, shuffle=False)\n",
    "\n",
    "# (Opcjonalnie) Przygotowanie zbioru ImageNet-1k, jeżeli dostępny na dysku:\n",
    "imagenet_train_dir = 'data/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/train'  # <-- Uwaga: ustawić poprawną ścieżkę jeśli dane dostępne\n",
    "imagenet_val_dir = 'data/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/val'\n",
    "imagenet_train_dataset = None\n",
    "imagenet_val_dataset = None\n",
    "if os.path.exists(imagenet_train_dir):\n",
    "    # Transformacje dla ImageNet: wymiary 224x224 jak w standardowych modelach\n",
    "    transform_simclr_imagenet = SimCLRTransform(input_size=224)\n",
    "    transform_dino_imagenet = DINOTransform()  # domyślne parametry dla DINO (224 global, 96 lokal)\n",
    "    transform_mae_imagenet = MAETransform()\n",
    "    # transformacje dla baseline i linear eval na ImageNet (przycięcie centralne dla val)\n",
    "    train_transform_supervised_imnet = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.2, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                             std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    val_transform_imnet = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                             std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    imagenet_val_dir_to_use = restructure_val_dir()\n",
    "\n",
    "    # Używamy ImageFolder do wczytania danych z katalogu\n",
    "    imagenet_train_dataset = torchvision.datasets.ImageFolder(root=imagenet_train_dir,\n",
    "                                                              transform=None)  # transform ustawimy dynamicznie\n",
    "    imagenet_val_dataset = torchvision.datasets.ImageFolder(root=imagenet_val_dir_to_use,\n",
    "                                                            transform=val_transform_imnet)\n",
    "    print(\"ImageNet datasets prepared.\")\n",
    "else:\n",
    "    print(\"ImageNet data not found, skipping ImageNet training in this run.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hussein/pytoniec/lib/python3.13/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restructuring validation directory...\n",
      "Validation directory restructured successfully.\n",
      "ImageNet datasets prepared.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-26T17:44:18.750421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import timm\n",
    "from lightly.loss import NTXentLoss, DINOLoss\n",
    "from lightly.models.modules.heads import SimCLRProjectionHead, BYOLProjectionHead, BYOLPredictionHead\n",
    "from lightly.models.utils import update_momentum, deactivate_requires_grad\n",
    "\n",
    "class DINOProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=2048, bottleneck_dim=256, freeze_last_layer=1):\n",
    "        super().__init__()\n",
    "        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, output_dim, bias=False))\n",
    "        self.last_layer.weight_g.requires_grad = False  # freeze weight normalization\n",
    "        self.freeze_last_layer = freeze_last_layer\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, bottleneck_dim),\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp(x)\n",
    "        x = nn.functional.normalize(x, dim=-1, p=2)\n",
    "        x = self.last_layer(x)\n",
    "        return x\n",
    "\n",
    "    def unfreeze_last_layer(self):\n",
    "        \"\"\"Unfreeze the last layer's weight normalization parameters\"\"\"\n",
    "        if hasattr(self.last_layer, 'weight_g'):\n",
    "            self.last_layer.weight_g.requires_grad = True\n",
    "\n",
    "\n",
    "# 2.1 Trenowanie masked autoencoder (MAE/SimMIM) na zbiorze nieoznaczonym\n",
    "def pretrain_masked_autoencoder(train_dataset, epochs=20, batch_size=128, lr=1.5e-4):\n",
    "    \"\"\"\n",
    "    Trenuje model typu Masked Autoencoder na podanym zbiorze danych.\n",
    "    Zwraca wytrenowany encoder (backbone) oraz cały model (encoder+decoder).\n",
    "    \"\"\"\n",
    "    # Ustawiamy transformację dla datasetu (MAETransform przygotowuje random crop i normalizację)\n",
    "    train_dataset.transform = transform_mae\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # Tworzymy model - ViT jako encoder, prosta warstwa liniowa jako decoder (SimMIM styl)\n",
    "    vit = torchvision.models.vit_b_16(pretrained=False)  # ViT-base patch16; dla CIFAR może być nadmiarowy\n",
    "    # Dostosowanie: zmieniamy rozmiar wejścia patch (CIFAR obraz 32x32, patch 16 -> 2x2 patchy, to za mało)\n",
    "    # Alternatywnie: powiększamy obrazy CIFAR do 224 wewnątrz transformacji by użyć ViT patch16.\n",
    "    # (Tutaj zakładamy, że transformacja MAETransform może wewnętrznie robić resize do 224; jeśli nie, warto dodać Resize(224) do transformacji dla CIFAR.)\n",
    "    # Budujemy model maskowanego autoenkodera:\n",
    "    class MaskedAutoencoder(nn.Module):\n",
    "        def __init__(self, vit_encoder):\n",
    "            super().__init__()\n",
    "            self.encoder = lightly.models.modules.masked_vision_transformer_torchvision.MaskedVisionTransformerTorchvision(vit=vit_encoder)\n",
    "            # Decoder: pojedyncza warstwa liniowa mapująca latent do rozmiaru patch (patch_size^2 * 3 kanały)\n",
    "            hidden_dim = vit_encoder.hidden_dim if hasattr(vit_encoder, 'hidden_dim') else vit_encoder.heads.head.in_features\n",
    "            patch_size = vit_encoder.patch_size if hasattr(vit_encoder, 'patch_size') else 16\n",
    "            self.decoder = nn.Linear(hidden_dim, patch_size**2 * 3)\n",
    "        def forward(self, images):\n",
    "            # Losowe maskowanie tokenów (75% tokenów maskowane)\n",
    "            batch_size = images.shape[0]\n",
    "            seq_len = self.encoder.seq_length  # długość sekwencji patchy (łącznie z tokenem cls jeśli jest)\n",
    "            # Generowanie maski losowej\n",
    "            idx_keep, idx_mask = lightly.models.utils.random_token_mask((batch_size, seq_len), mask_ratio=0.75, device=images.device)\n",
    "            # Encoder dostaje pełny obraz oraz indeksy maskowanych tokenów\n",
    "            encoded = self.encoder.encode(images=images, idx_mask=idx_mask)\n",
    "            # Wybieramy tylko reprezentacje niezamaskowanych tokenów (MAE tak robi przed dekoderem)\n",
    "            encoded_masked = lightly.models.utils.get_at_index(encoded, idx_mask)\n",
    "            # Dekoder: próba rekonstrukcji oryginalnych patchy dla maskowanych pozycji\n",
    "            preds = self.decoder(encoded_masked)  # wyniki dla maskowanych patchy\n",
    "            # Wyznaczamy \"ground truth\" - faktyczne piksele maskowanych patchy\n",
    "            patches = lightly.models.utils.patchify(images, patch_size)\n",
    "            # Uwaga: jeśli ViT dodaje token klas (cls token) to indeksy patchy trzeba zmodyfikować (idx_mask-1)\n",
    "            target = lightly.models.utils.get_at_index(patches, idx_mask - 1)\n",
    "            return preds, target\n",
    "\n",
    "    # Inicjalizacja modelu i ustawienie na urządzenie\n",
    "    mae_model = MaskedAutoencoder(vit).to(device)\n",
    "    # Funkcja kosztu - błąd L1 (średni bezwzględny) pomiędzy patchami\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.AdamW(mae_model.parameters(), lr=lr)\n",
    "    mae_model.train()\n",
    "    print(\">>> Trenowanie Masked Autoencoder przez {} epok...\".format(epochs))\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for (images, _) in train_loader:\n",
    "            images = images.to(device)\n",
    "            preds, targets = mae_model(images)        # forward prze maskowany autoenkoder\n",
    "            loss = criterion(preds, targets)          # oblicz stratę rekonstrukcji\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"[MAE] Epoka {epoch+1}/{epochs}, średni L1 loss: {avg_loss:.4f}\")\n",
    "    # Zwracamy wytrenowany encoder (vit) oraz cały model (encoder+decoder)\n",
    "    return mae_model.encoder.backbone, mae_model\n",
    "\n",
    "# 2.2 Trenowanie metody SimCLR (kontrastywna) na zbiorze nieoznaczonym\n",
    "def pretrain_simclr(train_dataset, epochs=20, batch_size=128, lr=6e-2):\n",
    "    \"\"\"\n",
    "    Trenuje model SimCLR (ResNet18 + projection head) na podanym zbiorze danych.\n",
    "    Zwraca wytrenowany backbone (ResNet bez klasyfikatora).\n",
    "    \"\"\"\n",
    "    # Ustawienie transformacji dwóch widoków na dataset\n",
    "    train_dataset.transform = transform_simclr\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # Tworzymy backbone (ResNet18) i usuwamy ostatnią warstwę klasyfikacji\n",
    "    resnet = torchvision.models.resnet18(pretrained=False)\n",
    "    backbone = nn.Sequential(*list(resnet.children())[:-1])  # do przedostatniej warstwy (global avg pool)\n",
    "    backbone_output_dim = resnet.fc.in_features  # wymiar cech wyjściowych backbone (512 dla ResNet18)\n",
    "    # Projekcyjna głowica SimCLR: MLP (hidden_dim -> output_dim=128 zazwyczaj)\n",
    "    projection_head = SimCLRProjectionHead(input_dim=backbone_output_dim, hidden_dim=backbone_output_dim, output_dim=128)\n",
    "    # Funkcja kosztu NT-Xent (InfoNCE) – Lightly ma implementację. Bez memory bank (tu SimCLR, używamy tylko batch negatywów).\n",
    "    criterion = NTXentLoss()  # domyślnie temperature=0.5\n",
    "    optimizer = torch.optim.SGD(list(backbone.parameters()) + list(projection_head.parameters()),\n",
    "                                lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    backbone.to(device)\n",
    "    projection_head.to(device)\n",
    "    backbone.train()\n",
    "    projection_head.train()\n",
    "    print(\">>> Trenowanie SimCLR przez {} epok...\".format(epochs))\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for (views, _) in train_loader:  # views to tupla (x_i, x_j) augmentacji\n",
    "            x1, x2 = views[0].to(device), views[1].to(device)\n",
    "            # Obliczamy reprezentacje h dla obu widoków\n",
    "            h1 = backbone(x1).flatten(start_dim=1)\n",
    "            h2 = backbone(x2).flatten(start_dim=1)\n",
    "            # Projekcja z MLP (z) - wektor 128-d do kontrastowania\n",
    "            z1 = projection_head(h1)\n",
    "            z2 = projection_head(h2)\n",
    "            # Oblicz strata NTXentLoss (przyjmuje 2 tensory: pozytywne pary)\n",
    "            loss = criterion(z1, z2)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"[SimCLR] Epoka {epoch+1}/{epochs}, średni loss: {avg_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "    return backbone\n",
    "\n",
    "# 2.2 Trenowanie metody MoCo (Momentum Contrast) na zbiorze nieoznaczonym\n",
    "def pretrain_moco(train_dataset, epochs=20, batch_size=128, lr=0.06, memory_bank_size=4096):\n",
    "    \"\"\"\n",
    "    Trenuje model MoCo v2 (ResNet18 z encoderem kluczy i kolejką) na podanym zbiorze danych.\n",
    "    Zwraca wytrenowany backbone (encoder zapytań).\n",
    "    \"\"\"\n",
    "    # Ustawienie transformacji dwóch widoków (tak jak SimCLR)\n",
    "    train_dataset.transform = transform_simclr\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # Backbone główny (query encoder) i momentum backbone (key encoder)\n",
    "    resnet = torchvision.models.resnet18(pretrained=False)\n",
    "    query_encoder = nn.Sequential(*list(resnet.children())[:-1])\n",
    "    backbone_output_dim = resnet.fc.in_features  # 512\n",
    "    # Kopiujemy encoder do key_encoder i zamrażamy gradienty w nim\n",
    "    key_encoder = copy.deepcopy(query_encoder)\n",
    "    deactivate_requires_grad(key_encoder)  # wyłączenie obliczania grad dla momentum encodera\n",
    "    # Projekcyjne głowice dla obu encoderów (MoCo v2 używa MLP jak SimCLR)\n",
    "    proj_q = SimCLRProjectionHead(input_dim=backbone_output_dim, hidden_dim=backbone_output_dim, output_dim=128)\n",
    "    proj_k = copy.deepcopy(proj_q)\n",
    "    deactivate_requires_grad(proj_k)\n",
    "    # Bufor (queue) na negatywne przykłady\n",
    "    # Inicjalizacja kolejki losowymi wektorami jednostkowymi\n",
    "    queue_size = memory_bank_size\n",
    "    feature_dim = 128\n",
    "    queue = F.normalize(torch.randn(queue_size, feature_dim, device=device), dim=1)\n",
    "    queue_ptr = 0  # wskaźnik pozycji do nadpisania w kolejce\n",
    "\n",
    "    # Funkcja strat InfoNCE (NTXentLoss) z memory bank, żeby wykorzystywać kolejkę jako negatywy\n",
    "    criterion = NTXentLoss(memory_bank_size=(memory_bank_size, feature_dim))\n",
    "    optimizer = torch.optim.SGD(list(query_encoder.parameters()) + list(proj_q.parameters()), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    query_encoder.to(device)\n",
    "    proj_q.to(device)\n",
    "    key_encoder.to(device)\n",
    "    proj_k.to(device)\n",
    "    query_encoder.train(); proj_q.train()\n",
    "    key_encoder.eval(); proj_k.eval()  # momentum encoder nie trenujemy (ewaluacja - nie aktualizuje BN)\n",
    "    print(\">>> Trenowanie MoCo przez {} epok...\".format(epochs))\n",
    "    momentum = 0.999  # współczynnik momentum do uaktualniania kluczowego encodera\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for (views, _) in train_loader:\n",
    "            x_q, x_k = views[0].to(device), views[1].to(device)  # pierwsza augmentacja jako query, druga jako key\n",
    "            # Forward przez query encoder\n",
    "            q_features = query_encoder(x_q).flatten(start_dim=1)\n",
    "            q_proj = F.normalize(proj_q(q_features), dim=1)  # znormalizowane z_q (128-dim)\n",
    "            # Forward przez key encoder (bez gradientu)\n",
    "            with torch.no_grad():\n",
    "                # Aktualizujemy parametry key_encoder z momentum (z każdą iteracją upodabniamy do query_encoder)\n",
    "                update_momentum(query_encoder, key_encoder, m=momentum)\n",
    "                update_momentum(proj_q, proj_k, m=momentum)\n",
    "                k_features = key_encoder(x_k).flatten(start_dim=1)\n",
    "                k_proj = F.normalize(proj_k(k_features), dim=1)  # z_k (target)\n",
    "            # Obliczamy podobieństwa z query do: pozytyw (k_proj tej samej próbki) oraz negatywy (wszystkie z kolejki)\n",
    "            # Lightly NTXentLoss z memory bank pozwala to uprościć: przekażemy q_proj i k_proj, gdzie k_proj zostanie dodany do memory bank automatycznie.\n",
    "            loss = criterion(q_proj, k_proj)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            # Uaktualnienie kolejki: dodajemy nowe k_proj do queue, zastępując najstarsze\n",
    "            batch_size_effective = k_proj.shape[0]\n",
    "            if batch_size_effective <= queue_size:\n",
    "                # nadpisz najstarsze elementy\n",
    "                queue[queue_ptr:queue_ptr+batch_size_effective, :] = k_proj.detach()\n",
    "                queue_ptr = (queue_ptr + batch_size_effective) % queue_size\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"[MoCo] Epoka {epoch+1}/{epochs}, średni loss: {avg_loss:.4f}\")\n",
    "        scheduler.step()\n",
    "    return query_encoder  # zwracamy tylko encoder zapytań (wytrenowany backbone)\n",
    "\n",
    "# 2.3 Trenowanie metody BYOL (Bootstrap Your Own Latent) na zbiorze nieoznaczonym\n",
    "def pretrain_byol(train_dataset, epochs=20, batch_size=128, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Trenuje model BYOL (ResNet18 online + target) na podanym zbiorze danych.\n",
    "    Zwraca wytrenowany backbone (online network).\n",
    "    \"\"\"\n",
    "    # Ustawienie transformacji dwóch widoków (BYOL używa podobnych augmentacji jak SimCLR, ewentualnie dodając solarization, tutaj korzystamy z SimCLRTransform)\n",
    "    train_dataset.transform = transform_simclr\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # Definiujemy backbone sieci online i tworzymy kopię do sieci target\n",
    "    resnet = torchvision.models.resnet18(pretrained=False)\n",
    "    online_backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "    backbone_output_dim = resnet.fc.in_features  # 512\n",
    "    target_backbone = copy.deepcopy(online_backbone)\n",
    "    deactivate_requires_grad(target_backbone)  # sieć target nie ma gradientów\n",
    "    # Projekcja (MLP) i predykcja dla sieci online, projekcja dla sieci target\n",
    "    online_proj = BYOLProjectionHead(input_dim=backbone_output_dim, hidden_dim=backbone_output_dim, output_dim=256)\n",
    "    online_pred = BYOLPredictionHead(input_dim=256, hidden_dim=256, output_dim=256)\n",
    "    target_proj = copy.deepcopy(online_proj)\n",
    "    deactivate_requires_grad(target_proj)\n",
    "    # Optymalizujemy tylko parametry online (backbone, proj, pred)\n",
    "    optimizer = torch.optim.Adam(list(online_backbone.parameters()) + list(online_proj.parameters()) + list(online_pred.parameters()), lr=lr)\n",
    "    online_backbone.to(device); online_proj.to(device); online_pred.to(device)\n",
    "    target_backbone.to(device); target_proj.to(device)\n",
    "    online_backbone.train(); online_proj.train(); online_pred.train()\n",
    "    target_backbone.eval(); target_proj.eval()\n",
    "    momentum = 0.996  # współczynnik momentum do uaktualniania target network\n",
    "    print(\">>> Trenowanie BYOL przez {} epok...\".format(epochs))\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for (views, _) in train_loader:\n",
    "            x_a, x_b = views[0].to(device), views[1].to(device)  # dwie augmentacje\n",
    "            # Forward przez online network dla obu widoków\n",
    "            feat_a = online_backbone(x_a).flatten(start_dim=1)\n",
    "            feat_b = online_backbone(x_b).flatten(start_dim=1)\n",
    "            proj_a = online_proj(feat_a)\n",
    "            proj_b = online_proj(feat_b)\n",
    "            pred_a = online_pred(proj_a)  # predykcja dla a\n",
    "            pred_b = online_pred(proj_b)  # predykcja dla b\n",
    "            # Forward przez target network (bez grad)\n",
    "            with torch.no_grad():\n",
    "                # momentum update target sieci\n",
    "                update_momentum(online_backbone, target_backbone, m=momentum)\n",
    "                update_momentum(online_proj, target_proj, m=momentum)\n",
    "                # (target_pred nie ma, bo target sieć kończy na projekcji)\n",
    "                target_feat_a = target_backbone(x_a).flatten(start_dim=1)\n",
    "                target_feat_b = target_backbone(x_b).flatten(start_dim=1)\n",
    "                target_proj_a = target_proj(target_feat_a)\n",
    "                target_proj_b = target_proj(target_feat_b)\n",
    "            # Normalizacja wektorów projekcji i predykcji\n",
    "            pred_a_norm = F.normalize(pred_a, dim=1)\n",
    "            pred_b_norm = F.normalize(pred_b, dim=1)\n",
    "            target_a_norm = F.normalize(target_proj_b.detach(), dim=1)  # UWAGA: pred_a porównujemy z target z drugiego widoku\n",
    "            target_b_norm = F.normalize(target_proj_a.detach(), dim=1)\n",
    "            # Obliczenie straty MSE pomiędzy znormalizowanymi predykcjami online a docelowymi reprezentacjami target\n",
    "            loss = 2 - 2 * (pred_a_norm * target_a_norm).sum(dim=1).mean() - 2 * (pred_b_norm * target_b_norm).sum(dim=1).mean()\n",
    "            # (powyższe to równoważnik: loss = MSE(pred_a_norm, target_a_norm) + MSE(pred_b_norm, target_b_norm))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"[BYOL] Epoka {epoch+1}/{epochs}, średni loss: {avg_loss:.4f}\")\n",
    "    return online_backbone\n",
    "\n",
    "# 2.3 Trenowanie metody DINO (Distillation with No Labels) na zbiorze nieoznaczonym\n",
    "def pretrain_dino(train_dataset, epochs=10, batch_size=128, lr=0.0005):\n",
    "    \"\"\"\n",
    "    Trenowanie modelu z wykorzystaniem metody DINO (self-distillation with no labels).\n",
    "    \"\"\"\n",
    "    # Setup dataloader with DINO transforms\n",
    "    train_dataset.transform = transform_dino\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Create a single shared backbone for both student and teacher\n",
    "    backbone = timm.create_model('resnet18', pretrained=False, num_classes=0)\n",
    "    backbone_output_dim = backbone.num_features  # Typically 512 for ResNet18\n",
    "\n",
    "    # Move backbone to device\n",
    "    backbone = backbone.to(device)\n",
    "\n",
    "    # Create separate projection heads for student and teacher\n",
    "    student_head = DINOProjectionHead(\n",
    "        input_dim=backbone_output_dim,\n",
    "        hidden_dim=512,\n",
    "        bottleneck_dim=256,\n",
    "        output_dim=2048,\n",
    "        freeze_last_layer=1\n",
    "    ).to(device)\n",
    "\n",
    "    # Create teacher head (without deepcopy)\n",
    "    teacher_head = DINOProjectionHead(\n",
    "        input_dim=backbone_output_dim,\n",
    "        hidden_dim=512,\n",
    "        bottleneck_dim=256,\n",
    "        output_dim=2048\n",
    "    ).to(device)\n",
    "\n",
    "    # Initialize teacher head with student head weights\n",
    "    teacher_head.load_state_dict(student_head.state_dict())\n",
    "\n",
    "    # Deactivate gradients for teacher head\n",
    "    deactivate_requires_grad(teacher_head)\n",
    "\n",
    "    # Optimizer for student components only (backbone + student_head)\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {'params': backbone.parameters()},\n",
    "        {'params': student_head.parameters()}\n",
    "    ], lr=lr)\n",
    "\n",
    "    # DINO loss\n",
    "    criterion = DINOLoss(\n",
    "        output_dim=2048,\n",
    "        warmup_teacher_temp_epochs=5,\n",
    "        teacher_temp=0.07,\n",
    "        student_temp=0.1,\n",
    "        warmup_teacher_temp=0.04,\n",
    "    ).to(device)\n",
    "\n",
    "    # Training loop\n",
    "    print(f\">>> Training DINO for {epochs} epochs...\")\n",
    "    for epoch in range(epochs):\n",
    "        backbone.train()\n",
    "        student_head.train()\n",
    "        teacher_head.eval()  # Teacher always in eval mode\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for views, _ in train_loader:\n",
    "            # Get global and local views\n",
    "            global_views = [view.to(device) for view in views[:2]]  # First two are global views\n",
    "            local_views = [view.to(device) for view in views[2:]]  # Rest are local views\n",
    "\n",
    "            # Process all views through student\n",
    "            student_output = []\n",
    "            for view in global_views + local_views:\n",
    "                features = backbone(view)\n",
    "                output = student_head(features)\n",
    "                student_output.append(output)\n",
    "\n",
    "            # Process only global views through teacher\n",
    "            teacher_output = []\n",
    "            with torch.no_grad():  # No gradients for teacher\n",
    "                for view in global_views:\n",
    "                    features = backbone(view)  # Using the SAME backbone\n",
    "                    output = teacher_head(features)\n",
    "                    teacher_output.append(output)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(student_output, teacher_output, epoch)\n",
    "\n",
    "            # Optimization step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update teacher head through EMA\n",
    "            momentum_val = 0.996  # Typical DINO momentum value\n",
    "            for param_t, param_s in zip(teacher_head.parameters(), student_head.parameters()):\n",
    "                param_t.data = momentum_val * param_t.data + (1 - momentum_val) * param_s.data\n",
    "\n",
    "            # For the last layer that might be frozen\n",
    "            if epoch >= student_head.freeze_last_layer:\n",
    "                student_head.unfreeze_last_layer()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Print epoch stats\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"DINO training completed.\")\n",
    "    return backbone\n",
    "\n",
    "\n",
    "# ---- Wykonanie treningów dla poszczególnych metod na CIFAR100 ----\n",
    "print(\"\\n=== Rozpoczęcie treningów SSL na CIFAR100 ===\")\n",
    "# # Etap podstawowy:\n",
    "# backbone_mae_cifar100, mae_model = pretrain_masked_autoencoder(train_dataset_cifar100, epochs=10, batch_size=128)\n",
    "# backbone_simclr_cifar100 = pretrain_simclr(train_dataset_cifar100, epochs=10, batch_size=128)\n",
    "# # Etap pośredni (dodatkowo trening MoCo i supervised baseline na CIFAR100):\n",
    "# backbone_moco_cifar100 = pretrain_moco(train_dataset_cifar100, epochs=10, batch_size=128)\n",
    "# supervised_model_cifar100 = train_supervised_classifier(train_dataset_cifar100, num_classes=100, epochs=10, batch_size=128)\n",
    "# backbone_supervised_cifar100 = nn.Sequential(*list(supervised_model_cifar100.children())[:-1])  # ekstrakcja backbone z modelu nadzorowanego\n",
    "# # Etap zaawansowany (self-distillation metody BYOL i DINO na CIFAR100):\n",
    "# backbone_byol_cifar100 = pretrain_byol(train_dataset_cifar100, epochs=10, batch_size=128)\n",
    "backbone_dino_cifar100 = pretrain_dino(train_dataset_cifar100, epochs=10, batch_size=128)\n",
    "print(\"=== Zakończono pretraining SSL na CIFAR100 ===\\n\")\n",
    "\n",
    "# (Opcjonalnie) Trenowanie na ImageNet-1k dla etapów pośredniego/zaawansowanego\n",
    "if imagenet_train_dataset is not None:\n",
    "    print(\"=== Rozpoczęcie treningów SSL na ImageNet-1k (skala demonstracyjna) ===\")\n",
    "    # Ustawiamy odpowiednie transformacje dla ImageNet i tworzymy DataLoader\n",
    "    imagenet_train_dataset.transform = transform_simclr_imagenet\n",
    "    imnet_loader = torch.utils.data.DataLoader(imagenet_train_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
    "    # Dla przykładu trenujemy SimCLR i BYOL na ImageNet kilka epok (w praktyce potrzeba znacznie więcej)\n",
    "    # backbone_simclr_imnet = pretrain_simclr(imagenet_train_dataset, epochs=2, batch_size=256, lr=0.1)\n",
    "    # backbone_moco_imnet = pretrain_moco(imagenet_train_dataset, epochs=2, batch_size=256, lr=0.1, memory_bank_size=65536)\n",
    "    backbone_byol_imnet = pretrain_byol(imagenet_train_dataset, epochs=2, batch_size=256, lr=1e-3)\n",
    "    backbone_dino_imnet = pretrain_dino(imagenet_train_dataset, epochs=2, batch_size=256, lr=1e-3)\n",
    "    # Dla ImageNet również można by przeprowadzić linear probing lub ewaluację na CIFAR, ale pomijamy dalsze szczegóły w tym kodzie demonstracyjnym.\n",
    "    print(\"=== Zakończono pretraining SSL na ImageNet-1k ===\\n\")"
   ],
   "id": "e1d5a6d3d26b4edf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Rozpoczęcie treningów SSL na CIFAR100 ===\n",
      ">>> Training DINO for 10 epochs...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T17:12:22.821515Z",
     "start_time": "2025-05-26T17:12:22.819360Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "23132e26ccdc7e98",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8a8207501294b85a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
