{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1ef1922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Użycie biblioteki Lightly do transformacji i komponentów SSL\n",
    "import pytorch_lightning as pl\n",
    "from lightly.transforms import SimCLRTransform, DINOTransform, MAETransform\n",
    "from lightly.transforms.moco_transform import MoCoV2Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e72c25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Wykrycie urządzenia do trenowania (GPU jeśli dostępne)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7486c1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(42)  # Ustawienie ziarna dla powtarzalności"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748278c",
   "metadata": {},
   "source": [
    "# 1. Przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92091594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cifar_stats(name=\"cifar100\", root=\"data\", batch_size=5000):\n",
    "    ds_class = getattr(torchvision.datasets, name.upper())\n",
    "    ds = ds_class(root, train=True, download=True, transform=transforms.ToTensor())\n",
    "    loader = torch.utils.data.DataLoader(ds, batch_size=batch_size, num_workers=2, shuffle=False)\n",
    "    ch_sum = torch.zeros(3)\n",
    "    ch_sum_sq = torch.zeros(3)\n",
    "    n_pixels = 0\n",
    "    for imgs, _ in loader:\n",
    "        b, c, h, w = imgs.shape\n",
    "        n_pixels += b * h * w\n",
    "        ch_sum    += imgs.sum(dim=[0,2,3])\n",
    "        ch_sum_sq += (imgs**2).sum(dim=[0,2,3])\n",
    "    mean = ch_sum / n_pixels\n",
    "    std  = torch.sqrt(ch_sum_sq / n_pixels - mean**2)\n",
    "    return mean.tolist(), std.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "197b11cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_mean, cifar_std = compute_cifar_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c9be8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageNet data not found, skipping ImageNet training in this run.\n"
     ]
    }
   ],
   "source": [
    "# --- Przygotowanie zbiorów danych CIFAR10 i CIFAR100 ---\n",
    "# Transformacje dla trenowania nadzorowanego (baseline i linear probe): \n",
    "# losowe przycięcie i odbicie (augmentacja) + normalizacja.\n",
    "train_transform_supervised = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar_mean, std=cifar_std)\n",
    "])\n",
    "\n",
    "# Transformacja dla zbioru testowego (tylko skalowanie do tensoru i normalizacja).\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar_mean, std=cifar_std)\n",
    "])\n",
    "\n",
    "# Transformacje dla metod samonadzorowanych:\n",
    "# - Dla SimCLR/MoCo/BYOL: dwie zaugmentowane wersje obrazu.\n",
    "transform_simclr = SimCLRTransform(input_size=32)   # input_size=32 dla CIFAR\n",
    "transform_moco = MoCoV2Transform(input_size=32)  # input_size=32 dla CIFAR\n",
    "# - Dla DINO: transformacja generująca 2 widoki globalne i 6 lokalnych (domyślnie).\n",
    "transform_dino = DINOTransform(global_crop_size=32, local_crop_size=16,  # dopasowanie do mniejszych obrazków\n",
    "                               global_crop_scale=(0.5, 1.0), local_crop_scale=(0.2, 0.5))\n",
    "# - Dla MAE/SimMIM: jedna widok z losowymi augmentacjami (proste augmentacje).\n",
    "transform_mae = MAETransform()\n",
    "\n",
    "# Ładowanie danych CIFAR100 (train i test)\n",
    "train_dataset_cifar100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True,\n",
    "                                                      transform=None)  # transform ustawimy później per metoda\n",
    "test_dataset_cifar100 = torchvision.datasets.CIFAR100(root='./data', train=False, download=True,\n",
    "                                                     transform=test_transform)\n",
    "# Ładowanie danych CIFAR10 (train i test)\n",
    "train_dataset_cifar10 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True,\n",
    "                                                    transform=None)\n",
    "test_dataset_cifar10 = torchvision.datasets.CIFAR10(root='./data', train=False, download=True,\n",
    "                                                   transform=test_transform)\n",
    "\n",
    "# Dataloader dla ewaluacji (testy) – tutaj wykorzystamy go do obliczania cech i ewaluacji\n",
    "test_loader_cifar100 = torch.utils.data.DataLoader(test_dataset_cifar100, batch_size=256, shuffle=False)\n",
    "test_loader_cifar10 = torch.utils.data.DataLoader(test_dataset_cifar10, batch_size=256, shuffle=False)\n",
    "\n",
    "# (Opcjonalnie) Przygotowanie zbioru ImageNet-1k, jeżeli dostępny na dysku:\n",
    "imagenet_train_dir = '/path/to/ImageNet/train'  # <-- Uwaga: ustawić poprawną ścieżkę jeśli dane dostępne\n",
    "imagenet_val_dir = '/path/to/ImageNet/val'\n",
    "imagenet_train_dataset = None\n",
    "imagenet_val_dataset = None\n",
    "if os.path.exists(imagenet_train_dir):\n",
    "    # Transformacje dla ImageNet: wymiary 224x224 jak w standardowych modelach\n",
    "    transform_simclr_imagenet = SimCLRTransform(input_size=224)\n",
    "    transform_dino_imagenet = DINOTransform()  # domyślne parametry dla DINO (224 global, 96 lokal)\n",
    "    transform_mae_imagenet = MAETransform()\n",
    "    # transformacje dla baseline i linear eval na ImageNet (przycięcie centralne dla val)\n",
    "    train_transform_supervised_imnet = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.2, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                             std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    val_transform_imnet = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                             std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    # Używamy ImageFolder do wczytania danych z katalogu\n",
    "    imagenet_train_dataset = torchvision.datasets.ImageFolder(root=imagenet_train_dir,\n",
    "                                                              transform=None)  # transform ustawimy dynamicznie\n",
    "    imagenet_val_dataset = torchvision.datasets.ImageFolder(root=imagenet_val_dir,\n",
    "                                                            transform=val_transform_imnet)\n",
    "    print(\"ImageNet datasets prepared.\")\n",
    "else:\n",
    "    print(\"ImageNet data not found, skipping ImageNet training in this run.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e2d0a",
   "metadata": {},
   "source": [
    "# 2. Definicje modeli i trenowanie metod self supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "010125eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.vision_transformer import vit_base_patch32_224\n",
    "\n",
    "from lightly.loss import NTXentLoss, DINOLoss\n",
    "from lightly.models.modules.heads import SimCLRProjectionHead, DINOProjectionHead, BYOLProjectionHead, BYOLPredictionHead,\\\n",
    "    MoCoProjectionHead\n",
    "from lightly.models import utils\n",
    "from lightly.models.utils import update_momentum, deactivate_requires_grad,\\\n",
    "    batch_shuffle, batch_unshuffle\n",
    "\n",
    "from lightly.models.modules import MAEDecoderTIMM, MaskedVisionTransformerTIMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f147b33",
   "metadata": {},
   "source": [
    "## 2.1. Trenowanie masked autoencoder (MAE/SimMIM) na zbiorze nieoznaczonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0887031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_masked_autoencoder(train_dataset, epochs=20, batch_size=128, lr=1.5e-4):\n",
    "    \"\"\"\n",
    "    Trenuje model typu Masked Autoencoder na podanym zbiorze danych.\n",
    "    Zwraca wytrenowany encoder (backbone) oraz cały model (encoder+decoder).\n",
    "    \"\"\"\n",
    "    # Ustawiamy transformację dla datasetu (MAETransform przygotowuje random crop i normalizację)\n",
    "    train_dataset.transform = transform_mae\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        drop_last=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    # Tworzymy model - ViT jako encoder, prosta warstwa liniowa jako decoder (SimMIM styl)\n",
    "    vit = vit_base_patch32_224(pretrained=False)\n",
    "    # Dostosowanie: zmieniamy rozmiar wejścia patch (CIFAR obraz 32x32, patch 16 -> 2x2 patchy, to za mało)\n",
    "    # Alternatywnie: powiększamy obrazy CIFAR do 224 wewnątrz transformacji by użyć ViT patch16.\n",
    "    # (Tutaj zakładamy, że transformacja MAETransform może wewnętrznie robić resize do 224; jeśli nie, warto dodać Resize(224) do transformacji dla CIFAR.)\n",
    "    # Budujemy model maskowanego autoenkodera:\n",
    "    class MAE(pl.LightningModule):\n",
    "        def __init__(self, vit, lr=1.5e-4):\n",
    "            super().__init__()\n",
    "\n",
    "            decoder_dim = 512\n",
    "            self.mask_ratio = 0.75\n",
    "            self.patch_size = vit.patch_embed.patch_size[0]\n",
    "            self.lr = lr\n",
    "            self.backbone = MaskedVisionTransformerTIMM(vit=vit)\n",
    "            self.sequence_length = self.backbone.sequence_length\n",
    "            self.decoder = MAEDecoderTIMM(\n",
    "                num_patches=vit.patch_embed.num_patches,\n",
    "                patch_size=self.patch_size,\n",
    "                embed_dim=vit.embed_dim,\n",
    "                decoder_embed_dim=decoder_dim,\n",
    "                decoder_depth=1,\n",
    "                decoder_num_heads=16,\n",
    "                mlp_ratio=4.0,\n",
    "                proj_drop_rate=0.0,\n",
    "                attn_drop_rate=0.0,\n",
    "            )\n",
    "            self.criterion = nn.MSELoss()\n",
    "\n",
    "        def forward_encoder(self, images, idx_keep=None):\n",
    "            return self.backbone.encode(images=images, idx_keep=idx_keep)\n",
    "\n",
    "        def forward_decoder(self, x_encoded, idx_keep, idx_mask):\n",
    "            # build decoder input\n",
    "            batch_size = x_encoded.shape[0]\n",
    "            x_decode = self.decoder.embed(x_encoded)\n",
    "            x_masked = utils.repeat_token(\n",
    "                self.decoder.mask_token, (batch_size, self.sequence_length)\n",
    "            )\n",
    "            x_masked = utils.set_at_index(x_masked, idx_keep, x_decode.type_as(x_masked))\n",
    "\n",
    "            # decoder forward pass\n",
    "            x_decoded = self.decoder.decode(x_masked)\n",
    "\n",
    "            # predict pixel values for masked tokens\n",
    "            x_pred = utils.get_at_index(x_decoded, idx_mask)\n",
    "            x_pred = self.decoder.predict(x_pred)\n",
    "            return x_pred\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "            views = batch[0]\n",
    "            images = views[0]  # views contains only a single view\n",
    "            batch_size = images.shape[0]\n",
    "            idx_keep, idx_mask = utils.random_token_mask(\n",
    "                size=(batch_size, self.sequence_length),\n",
    "                mask_ratio=self.mask_ratio,\n",
    "                device=images.device,\n",
    "            )\n",
    "            x_encoded = self.forward_encoder(images=images, idx_keep=idx_keep)\n",
    "            x_pred = self.forward_decoder(\n",
    "                x_encoded=x_encoded, idx_keep=idx_keep, idx_mask=idx_mask\n",
    "            )\n",
    "\n",
    "            # get image patches for masked tokens\n",
    "            patches = utils.patchify(images, self.patch_size)\n",
    "            # must adjust idx_mask for missing class token\n",
    "            target = utils.get_at_index(patches, idx_mask - 1)\n",
    "\n",
    "            loss = self.criterion(x_pred, target)\n",
    "            # Log metrics\n",
    "            self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "            return loss\n",
    "\n",
    "        def configure_optimizers(self):\n",
    "            optim = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "            return optim\n",
    "\n",
    "\n",
    "    # Inicjalizacja modelu i ustawienie na urządzenie\n",
    "    mae_model = MAE(vit=vit, lr=lr)\n",
    "\n",
    "    accelerator = 'gpu' if device == 'cuda' else 'cpu'\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=epochs,\n",
    "        accelerator=accelerator,\n",
    "        devices=1,\n",
    "        precision='16-mixed' if device == 'cuda' else 32,\n",
    "        log_every_n_steps=20,\n",
    "        enable_checkpointing= True,\n",
    "    )\n",
    "\n",
    "    print(\">>> Trenowanie Masked Autoencoder przez {} epok...\".format(epochs))\n",
    "    trainer.fit(mae_model, train_loader)\n",
    "\n",
    "    return mae_model.backbone, mae_model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ba9c21",
   "metadata": {},
   "source": [
    "## 2.2. Kontrastywne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6567ebe1",
   "metadata": {},
   "source": [
    "### 2.2.1. Trenowanie metody SimCLR (kontrastywna) na zbiorze nieoznaczonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efab14e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_simclr(train_dataset, epochs=20, batch_size=128, lr=6e-2):\n",
    "    \"\"\"\n",
    "    Trenuje model SimCLR (ResNet18 + projection head) na podanym zbiorze danych.\n",
    "    Zwraca wytrenowany backbone (ResNet bez klasyfikatora).\n",
    "    \"\"\"\n",
    "    # Ustawienie transformacji dwóch widoków na dataset\n",
    "    train_dataset.transform = transform_simclr\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        drop_last=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "    \n",
    "    class SimCLR(pl.LightningModule):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # create a ResNet backbone and remove the classification head\n",
    "            resnet = torchvision.models.resnet18(pretrained=False)\n",
    "            self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "            hidden_dim = resnet.fc.in_features\n",
    "            self.projection_head = SimCLRProjectionHead(hidden_dim, hidden_dim, 128)\n",
    "\n",
    "            self.criterion = NTXentLoss()\n",
    "\n",
    "        def forward(self, x):\n",
    "            h = self.backbone(x).flatten(start_dim=1)\n",
    "            z = self.projection_head(h)\n",
    "            return z\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "            (x0, x1), _, _ = batch\n",
    "            z0 = self.forward(x0)\n",
    "            z1 = self.forward(x1)\n",
    "            loss = self.criterion(z0, z1)\n",
    "            self.log(\"train_loss\", loss)\n",
    "            return loss\n",
    "\n",
    "        def configure_optimizers(self):\n",
    "            optim = torch.optim.SGD(\n",
    "                self.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4\n",
    "            )\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, epochs)\n",
    "            return [optim], [scheduler]\n",
    "        \n",
    "\n",
    "    simclr_model = SimCLR()\n",
    "    accelerator = 'gpu' if device == 'cuda' else 'cpu'\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=epochs,\n",
    "        devices=1,\n",
    "        accelerator=accelerator,\n",
    "        precision='16-mixed' if device == 'cuda' else 32,\n",
    "        log_every_n_steps=20,\n",
    "        enable_checkpointing=True,\n",
    "    )\n",
    "\n",
    "    print(\">>> Trenowanie SimCLR przez {} epok...\".format(epochs))\n",
    "    trainer.fit(simclr_model, train_loader)\n",
    "    return simclr_model.backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5577c666",
   "metadata": {},
   "source": [
    "### 2.2.2. Trenowanie metody MoCo (Momentum Contrast) na zbiorze nieoznaczonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0697502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_moco(train_dataset, epochs=20, batch_size=128, lr=0.06, memory_bank_size=4096):\n",
    "    \"\"\"\n",
    "    Trenuje model MoCo v2 (ResNet18 z encoderem kluczy i kolejką) na podanym zbiorze danych.\n",
    "    Zwraca wytrenowany backbone (encoder zapytań).\n",
    "    \"\"\"\n",
    "    # Ustawienie transformacji dwóch widoków (tak jak SimCLR)\n",
    "    train_dataset.transform = transform_moco\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        drop_last=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    " \n",
    "    class MoCo(pl.LightningModule):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            resnet = torchvision.models.resnet18()\n",
    "            self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "            hidden_dim = resnet.fc.in_features\n",
    "            self.projection_head = MoCoProjectionHead(hidden_dim, hidden_dim, 128)\n",
    "\n",
    "            self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "            self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "\n",
    "            deactivate_requires_grad(self.backbone_momentum)\n",
    "            deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "            self.criterion = NTXentLoss(memory_bank_size=(memory_bank_size, 128))\n",
    "\n",
    "        def forward(self, x):\n",
    "            query = self.backbone(x).flatten(start_dim=1)\n",
    "            query = self.projection_head(query)\n",
    "            return query\n",
    "\n",
    "        def forward_momentum(self, x):\n",
    "            key = self.backbone_momentum(x).flatten(start_dim=1)\n",
    "            key = self.projection_head_momentum(key).detach()\n",
    "            return key\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "                (x_q, x_k), _, _ = batch\n",
    "\n",
    "                # update momentum\n",
    "                update_momentum(self.backbone, self.backbone_momentum, 0.99)\n",
    "                update_momentum(self.projection_head, self.projection_head_momentum, 0.99)\n",
    "\n",
    "                # get queries\n",
    "                q = self.backbone(x_q).flatten(start_dim=1)\n",
    "                q = self.projection_head(q)\n",
    "\n",
    "                # get keys\n",
    "                k, shuffle = batch_shuffle(x_k)\n",
    "                k = self.backbone_momentum(k).flatten(start_dim=1)\n",
    "                k = self.projection_head_momentum(k)\n",
    "                k = batch_unshuffle(k, shuffle)\n",
    "\n",
    "                loss = self.criterion(q, k)\n",
    "                self.log(\"train_loss\", loss)\n",
    "                return loss\n",
    "\n",
    "        def on_train_epoch_end(self):\n",
    "            self.custom_histogram_weights()\n",
    "\n",
    "        # We provide a helper method to log weights in tensorboard\n",
    "        # which is useful for debugging.\n",
    "        def custom_histogram_weights(self):\n",
    "            for name, params in self.named_parameters():\n",
    "                self.logger.experiment.add_histogram(name, params, self.current_epoch)\n",
    "\n",
    "        def configure_optimizers(self):\n",
    "            optim = torch.optim.SGD(\n",
    "                self.parameters(),\n",
    "                lr=lr,\n",
    "                momentum=0.9,\n",
    "                weight_decay=5e-4,\n",
    "            )\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, epochs)\n",
    "            return [optim], [scheduler]        \n",
    "\n",
    "    moco_model = MoCo()\n",
    "    accelerator = 'gpu' if device == 'cuda' else 'cpu'\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=epochs,\n",
    "        devices=1,\n",
    "        accelerator=accelerator,\n",
    "        precision='16-mixed' if device == 'cuda' else 32,\n",
    "        log_every_n_steps=20,\n",
    "        enable_checkpointing=True,\n",
    "    )\n",
    "\n",
    "    print(\">>> Trenowanie MoCo v2 przez {} epok...\".format(epochs))\n",
    "    trainer.fit(moco_model, train_loader)\n",
    "    return moco_model.backbone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b4964",
   "metadata": {},
   "source": [
    "## 2.3. Self-distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d9c94c",
   "metadata": {},
   "source": [
    "### 2.3.1. Trenowanie metody BYOL (Bootstrap Your Own Latent) na zbiorze nieoznaczonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "730ae99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_byol(train_dataset, epochs=20, batch_size=128, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Trenuje model BYOL (ResNet18 online + target) na podanym zbiorze danych.\n",
    "    Zwraca wytrenowany backbone (online network).\n",
    "    \"\"\"\n",
    "    # Ustawienie transformacji dwóch widoków (BYOL używa podobnych augmentacji jak SimCLR, ewentualnie dodając solarization, tutaj korzystamy z SimCLRTransform)\n",
    "    train_dataset.transform = transform_simclr\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # Definiujemy backbone sieci online i tworzymy kopię do sieci target\n",
    "    resnet = torchvision.models.resnet18(pretrained=False)\n",
    "    online_backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "    backbone_output_dim = resnet.fc.in_features  # 512\n",
    "    target_backbone = copy.deepcopy(online_backbone)\n",
    "    deactivate_requires_grad(target_backbone)  # sieć target nie ma gradientów\n",
    "    # Projekcja (MLP) i predykcja dla sieci online, projekcja dla sieci target\n",
    "    online_proj = BYOLProjectionHead(input_dim=backbone_output_dim, hidden_dim=backbone_output_dim, output_dim=256)\n",
    "    online_pred = BYOLPredictionHead(input_dim=256, hidden_dim=256, output_dim=256)\n",
    "    target_proj = copy.deepcopy(online_proj)\n",
    "    deactivate_requires_grad(target_proj)\n",
    "    # Optymalizujemy tylko parametry online (backbone, proj, pred)\n",
    "    optimizer = torch.optim.Adam(list(online_backbone.parameters()) + list(online_proj.parameters()) + list(online_pred.parameters()), lr=lr)\n",
    "    online_backbone.to(device); online_proj.to(device); online_pred.to(device)\n",
    "    target_backbone.to(device); target_proj.to(device)\n",
    "    online_backbone.train(); online_proj.train(); online_pred.train()\n",
    "    target_backbone.eval(); target_proj.eval()\n",
    "    momentum = 0.996  # współczynnik momentum do uaktualniania target network\n",
    "    print(\">>> Trenowanie BYOL przez {} epok...\".format(epochs))\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for (views, _) in train_loader:\n",
    "            x_a, x_b = views[0].to(device), views[1].to(device)  # dwie augmentacje\n",
    "            # Forward przez online network dla obu widoków\n",
    "            feat_a = online_backbone(x_a).flatten(start_dim=1)\n",
    "            feat_b = online_backbone(x_b).flatten(start_dim=1)\n",
    "            proj_a = online_proj(feat_a)\n",
    "            proj_b = online_proj(feat_b)\n",
    "            pred_a = online_pred(proj_a)  # predykcja dla a\n",
    "            pred_b = online_pred(proj_b)  # predykcja dla b\n",
    "            # Forward przez target network (bez grad)\n",
    "            with torch.no_grad():\n",
    "                # momentum update target sieci\n",
    "                update_momentum(online_backbone, target_backbone, m=momentum)\n",
    "                update_momentum(online_proj, target_proj, m=momentum)\n",
    "                # (target_pred nie ma, bo target sieć kończy na projekcji)\n",
    "                target_feat_a = target_backbone(x_a).flatten(start_dim=1)\n",
    "                target_feat_b = target_backbone(x_b).flatten(start_dim=1)\n",
    "                target_proj_a = target_proj(target_feat_a)\n",
    "                target_proj_b = target_proj(target_feat_b)\n",
    "            # Normalizacja wektorów projekcji i predykcji\n",
    "            pred_a_norm = F.normalize(pred_a, dim=1)\n",
    "            pred_b_norm = F.normalize(pred_b, dim=1)\n",
    "            target_a_norm = F.normalize(target_proj_b.detach(), dim=1)  # UWAGA: pred_a porównujemy z target z drugiego widoku\n",
    "            target_b_norm = F.normalize(target_proj_a.detach(), dim=1)\n",
    "            # Obliczenie straty MSE pomiędzy znormalizowanymi predykcjami online a docelowymi reprezentacjami target\n",
    "            loss = 2 - 2 * (pred_a_norm * target_a_norm).sum(dim=1).mean() - 2 * (pred_b_norm * target_b_norm).sum(dim=1).mean()\n",
    "            # (powyższe to równoważnik: loss = MSE(pred_a_norm, target_a_norm) + MSE(pred_b_norm, target_b_norm))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"[BYOL] Epoka {epoch+1}/{epochs}, średni loss: {avg_loss:.4f}\")\n",
    "    return online_backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c77fa0",
   "metadata": {},
   "source": [
    "### 2.3.2. Trenowanie metody DINO (Distillation with No Labels) na zbiorze nieoznaczonym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3157c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_dino(train_dataset, epochs=20, batch_size=128, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Trenuje model DINO (ResNet18 student + momentum teacher) na podanym zbiorze danych.\n",
    "    Zwraca wytrenowany backbone (student backbone).\n",
    "    \"\"\"\n",
    "    # Ustawienie transformacji DINO (wiele widoków) na dataset\n",
    "    train_dataset.transform = transform_dino\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # Definiujemy backbone (ResNet18) dla studenta i kopiujemy dla nauczyciela\n",
    "    resnet = torchvision.models.resnet18(pretrained=False)\n",
    "    student_backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "    backbone_output_dim = resnet.fc.in_features  # 512\n",
    "    teacher_backbone = copy.deepcopy(student_backbone)\n",
    "    deactivate_requires_grad(teacher_backbone)\n",
    "    # Głowice projekcyjne DINO dla studenta i nauczyciela.\n",
    "    # Używamy DINOProjectionHead: parametry (in_dim, hidden_dim, bottleneck_dim, out_dim, [opcje])\n",
    "    student_head = DINOProjectionHead(input_dim=backbone_output_dim, hidden_dim=512, bottleneck_dim=256, output_dim=2048, freeze_last_layer=1)\n",
    "    teacher_head = copy.deepcopy(student_head)\n",
    "    # teacher_head nie zamrażamy w całości, ale podczas optymalizacji nie będziemy go aktualizować (nie jest w optimizer)\n",
    "    deactivate_requires_grad(teacher_head)\n",
    "    # Funkcja kosztu DINO – porównuje wyjścia nauczyciela i studenta (zawiera mechanizm centrowania i temperatury)\n",
    "    criterion = DINOLoss(output_dim=2048, warmup_teacher_temp_epochs=5)\n",
    "    # Optimizer tylko dla sieci studenta (backbone + head)\n",
    "    optimizer = torch.optim.Adam(list(student_backbone.parameters()) + list(student_head.parameters()), lr=lr)\n",
    "    # Harmonogram zmiany współczynnika momentum (od nieco mniejszego do 1)\n",
    "    # Będziemy liniowo zwiększać momentum nauczyciela od 0.996 do 1.0 w trakcie epok\n",
    "    initial_momentum = 0.996\n",
    "    final_momentum = 1.0\n",
    "    student_backbone.to(device); student_head.to(device)\n",
    "    teacher_backbone.to(device); teacher_head.to(device)\n",
    "    student_backbone.train(); student_head.train()\n",
    "    teacher_backbone.eval(); teacher_head.eval()\n",
    "    print(\">>> Trenowanie DINO przez {} epok...\".format(epochs))\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        # Wyznacz wartość momentum dla bieżącej epoki (cosine schedule)\n",
    "        momentum_val = initial_momentum + (final_momentum - initial_momentum) * (epoch / (epochs - 1))\n",
    "        for batch in train_loader:\n",
    "            views = batch[0]  # lista widoków augmentowanych (list length = 2 + n_local_views, domyślnie 8)\n",
    "            # Uaktualnienie wag nauczyciela (backbone i head) - momentum update przed forward\n",
    "            update_momentum(student_backbone, teacher_backbone, m=momentum_val)\n",
    "            update_momentum(student_head, teacher_head, m=momentum_val)\n",
    "            # Przenieś wszystkie widoki na GPU\n",
    "            views = [v.to(device) for v in views]\n",
    "            # DINO: nauczyciel otrzymuje tylko 2 globalne widoki (zwykle 224x224), student wszystkie\n",
    "            # Zakładamy, że transform_dino generuje 2 pierwsze widoki jako \"globalne\"\n",
    "            global_views = views[:2]\n",
    "            # Obliczenia forward:\n",
    "            # - Teacher output dla globalnych widoków (zatrzymujemy gradient)\n",
    "            with torch.no_grad():\n",
    "                teacher_out = [teacher_head(teacher_backbone(v).flatten(start_dim=1)) for v in global_views]\n",
    "            # - Student output dla wszystkich widoków\n",
    "            student_out = [student_head(student_backbone(v).flatten(start_dim=1)) for v in views]\n",
    "            # Oblicz stratę DINO (porównuje rozkłady wyjściowe teacher vs student)\n",
    "            loss = criterion(teacher_out, student_out, epoch=epoch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # DINO zaleca zablokowanie gradientów ostatniej warstwy projekcyjnej studenta na wczesnych epokach (freeze_last_layer)\n",
    "            student_head.cancel_last_layer_gradients(current_epoch=epoch)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"[DINO] Epoka {epoch+1}/{epochs}, średni loss: {avg_loss:.4f}\")\n",
    "    return student_backbone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7169e8",
   "metadata": {},
   "source": [
    "## 2.4. Model nadzorowany (supervised) na CIFAR100 (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f9c99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_supervised_classifier(train_dataset, num_classes=100, epochs=20, batch_size=128, lr=0.1):\n",
    "    \"\"\"\n",
    "    Trenuje model klasyfikacyjny (ResNet18) w sposób nadzorowany na podanym zbiorze (z etykietami).\n",
    "    Zwraca wytrenowany model (backbone + klasyfikator).\n",
    "    \"\"\"\n",
    "    # Ustawiamy transformacje augmentacyjne dla treningu nadzorowanego\n",
    "    train_dataset.transform = train_transform_supervised\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    # Tworzymy model ResNet18 z random inicjalizacją (num_classes wyjściowych)\n",
    "    model = torchvision.models.resnet18(pretrained=False, num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    model.train()\n",
    "    print(\">>> Trenowanie modelu nadzorowanego (ResNet18) przez {} epok...\".format(epochs))\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for (images, labels) in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            # Obliczanie dokładności bieżącej partii (dla monitorowania)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        acc = 100.0 * correct / total\n",
    "        print(f\"[Supervised] Epoka {epoch+1}/{epochs}, loss: {avg_loss:.4f}, accuracy: {acc:.2f}%\")\n",
    "        scheduler.step()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880de3bc",
   "metadata": {},
   "source": [
    "## 2.5. Trenowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22d01672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Rozpoczęcie treningów SSL na CIFAR100 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Trenowanie Masked Autoencoder przez 10 epok...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type                        | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | backbone  | MaskedVisionTransformerTIMM | 88.2 M | train\n",
      "1 | decoder   | MAEDecoderTIMM              | 5.1 M  | train\n",
      "2 | criterion | MSELoss                     | 0      | train\n",
      "------------------------------------------------------------------\n",
      "93.3 M    Trainable params\n",
      "64.0 K    Non-trainable params\n",
      "93.4 M    Total params\n",
      "373.497   Total estimated model params size (MB)\n",
      "292       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 195/195 [01:08<00:00,  2.84it/s, v_num=9, train_loss_step=0.281, train_loss_epoch=0.292]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 195/195 [01:22<00:00,  2.37it/s, v_num=9, train_loss_step=0.281, train_loss_epoch=0.292]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | backbone        | Sequential           | 11.2 M | train\n",
      "1 | projection_head | SimCLRProjectionHead | 328 K  | train\n",
      "2 | criterion       | NTXentLoss           | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "46.022    Total estimated model params size (MB)\n",
      "77        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Trenowanie SimCLR przez 10 epok...\n",
      "Epoch 0:   0%|          | 0/195 [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Etap podstawowy:\u001b[39;00m\n\u001b[32m      4\u001b[39m backbone_mae_cifar100, mae_model = pretrain_masked_autoencoder(train_dataset_cifar100, epochs=\u001b[32m10\u001b[39m, batch_size=\u001b[32m256\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m backbone_simclr_cifar100 = \u001b[43mpretrain_simclr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset_cifar100\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Etap pośredni (dodatkowo trening MoCo i supervised baseline na CIFAR100):\u001b[39;00m\n\u001b[32m      7\u001b[39m backbone_moco_cifar100 = pretrain_moco(train_dataset_cifar100, epochs=\u001b[32m10\u001b[39m, batch_size=\u001b[32m256\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mpretrain_simclr\u001b[39m\u001b[34m(train_dataset, epochs, batch_size, lr)\u001b[39m\n\u001b[32m     53\u001b[39m trainer = pl.Trainer(\n\u001b[32m     54\u001b[39m     max_epochs=epochs,\n\u001b[32m     55\u001b[39m     devices=\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m     enable_checkpointing=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     60\u001b[39m )\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m>>> Trenowanie SimCLR przez \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m epok...\u001b[39m\u001b[33m\"\u001b[39m.format(epochs))\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimclr_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m simclr_model.backbone\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1054\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_epoch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:150\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m         \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:320\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_batch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m         batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    322\u001b[39m         batch_output = \u001b[38;5;28mself\u001b[39m.manual_optimization.run(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         closure()\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_ready()\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[32m    280\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_completed()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m pl_module._current_fx_name = hook_name\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    179\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1302\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimizer_step\u001b[39m(\n\u001b[32m   1272\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1273\u001b[39m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1276\u001b[39m     optimizer_closure: Optional[Callable[[], Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1277\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1278\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1279\u001b[39m \u001b[33;03m    the optimizer.\u001b[39;00m\n\u001b[32m   1280\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1300\u001b[39m \n\u001b[32m   1301\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1302\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\amp.py:79\u001b[39m, in \u001b[36mMixedPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(optimizer, LBFGS):\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# If backward was skipped in automatic optimization (return None), unscaling is not needed\u001b[39;00m\n\u001b[32m     82\u001b[39m skip_unscaling = closure_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m model.automatic_optimization\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:131\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@torch\u001b[39m.enable_grad()\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> ClosureResult:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    134\u001b[39m         \u001b[38;5;28mself\u001b[39m.warning_cache.warn(\u001b[33m\"\u001b[39m\u001b[33m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:319\u001b[39m, in \u001b[36m_AutomaticOptimization._training_step\u001b[39m\u001b[34m(self, kwargs)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[32m    309\u001b[39m \n\u001b[32m    310\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    315\u001b[39m \n\u001b[32m    316\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    317\u001b[39m trainer = \u001b[38;5;28mself\u001b[39m.trainer\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m training_step_output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.strategy.post_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer.world_size > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    331\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:391\u001b[39m, in \u001b[36mStrategy.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mtraining_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mpretrain_simclr.<locals>.SimCLR.training_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     (x0, x1), _, _ = batch\n\u001b[32m     36\u001b[39m     z0 = \u001b[38;5;28mself\u001b[39m.forward(x0)\n\u001b[32m     37\u001b[39m     z1 = \u001b[38;5;28mself\u001b[39m.forward(x1)\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 3, got 2)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/390 [27:32<?, ?it/s, v_num=9, train_loss_step=0.347, train_loss_epoch=0.355]\n",
      "Epoch 4:   0%|          | 0/195 [18:32<?, ?it/s, v_num=9, train_loss_step=0.330, train_loss_epoch=0.360]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000026D744B1260>\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1663, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"e:\\programowanie\\studia\\sem6\\WB2\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1621, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- Wykonanie treningów dla poszczególnych metod na CIFAR100 ----\n",
    "print(\"\\n=== Rozpoczęcie treningów SSL na CIFAR100 ===\")\n",
    "# Etap podstawowy:\n",
    "backbone_mae_cifar100, mae_model = pretrain_masked_autoencoder(train_dataset_cifar100, epochs=10, batch_size=256)\n",
    "backbone_simclr_cifar100 = pretrain_simclr(train_dataset_cifar100, epochs=10, batch_size=256)\n",
    "# Etap pośredni (dodatkowo trening MoCo i supervised baseline na CIFAR100):\n",
    "backbone_moco_cifar100 = pretrain_moco(train_dataset_cifar100, epochs=10, batch_size=256)\n",
    "# supervised_model_cifar100 = train_supervised_classifier(train_dataset_cifar100, num_classes=100, epochs=10, batch_size=128)\n",
    "# backbone_supervised_cifar100 = nn.Sequential(*list(supervised_model_cifar100.children())[:-1])  # ekstrakcja backbone z modelu nadzorowanego\n",
    "# Etap zaawansowany (self-distillation metody BYOL i DINO na CIFAR100):\n",
    "# backbone_byol_cifar100 = pretrain_byol(train_dataset_cifar100, epochs=10, batch_size=128)\n",
    "# backbone_dino_cifar100 = pretrain_dino(train_dataset_cifar100, epochs=10, batch_size=128)\n",
    "print(\"=== Zakończono pretraining SSL na CIFAR100 ===\\n\")\n",
    "\n",
    "# (Opcjonalnie) Trenowanie na ImageNet-1k dla etapów pośredniego/zaawansowanego\n",
    "if imagenet_train_dataset is not None:\n",
    "    print(\"=== Rozpoczęcie treningów SSL na ImageNet-1k (skala demonstracyjna) ===\")\n",
    "    # Ustawiamy odpowiednie transformacje dla ImageNet i tworzymy DataLoader\n",
    "    imagenet_train_dataset.transform = transform_simclr_imagenet\n",
    "    imnet_loader = torch.utils.data.DataLoader(imagenet_train_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
    "    # Dla przykładu trenujemy SimCLR i BYOL na ImageNet kilka epok (w praktyce potrzeba znacznie więcej)\n",
    "    backbone_simclr_imnet = pretrain_simclr(imagenet_train_dataset, epochs=2, batch_size=256, lr=0.1)\n",
    "    backbone_moco_imnet = pretrain_moco(imagenet_train_dataset, epochs=2, batch_size=256, lr=0.1, memory_bank_size=65536)\n",
    "    # backbone_byol_imnet = pretrain_byol(imagenet_train_dataset, epochs=2, batch_size=256, lr=1e-3)\n",
    "    # backbone_dino_imnet = pretrain_dino(imagenet_train_dataset, epochs=2, batch_size=256, lr=1e-3)\n",
    "    # Dla ImageNet również można by przeprowadzić linear probing lub ewaluację na CIFAR, ale pomijamy dalsze szczegóły w tym kodzie demonstracyjnym.\n",
    "    print(\"=== Zakończono pretraining SSL na ImageNet-1k ===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c715d",
   "metadata": {},
   "source": [
    "# 3. Ewaluacja reprezentacji -- linear probing i KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0aa8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja do ewaluacji linear probing: trenowanie liniowego klasyfikatora na zamrożonym backbone\n",
    "def evaluate_linear(backbone, train_dataset, test_dataset, num_classes, epochs=5, lr=0.01):\n",
    "    \"\"\"\n",
    "    Trenuje liniowy klasyfikator (1 warstwa) na cechach zadanego backbone (który pozostaje zamrożony).\n",
    "    Zwraca accuracy [%] na zbiorze testowym.\n",
    "    \"\"\"\n",
    "    # Zamrażamy backbone\n",
    "    backbone.eval()\n",
    "    for param in backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Definiujemy prosty model: backbone (zamrożony) + linear layer\n",
    "    class LinearModel(nn.Module):\n",
    "        def __init__(self, backbone, num_classes):\n",
    "            super().__init__()\n",
    "            self.backbone = backbone\n",
    "            # Warstwa liniowa biorąca wektor cech (np. 512) na klasy\n",
    "            self.fc = nn.Linear(backbone_output_dim, num_classes)\n",
    "        def forward(self, x):\n",
    "            with torch.no_grad():\n",
    "                feats = self.backbone(x).flatten(start_dim=1)  # cechy ze backbone\n",
    "            out = self.fc(feats)\n",
    "            return out\n",
    "    # Określamy wymiar wyjściowy backbone (zakładamy że znamy, np. 512 dla ResNet18, lub pobieramy automatycznie)\n",
    "    backbone_output_dim = None\n",
    "    # Spróbujmy wyznaczyć wymiar cech backbone poprzez przepuszczenie jednego batcha testowego\n",
    "    backbone_output_dim = None\n",
    "    for (images, _) in test_loader_cifar100:  # używamy CIFAR100 test loader by nie modyfikować datasetu\n",
    "        with torch.no_grad():\n",
    "            feat = backbone(images.to(device)).flatten(start_dim=1)\n",
    "        backbone_output_dim = feat.shape[1]\n",
    "        break\n",
    "    if backbone_output_dim is None:\n",
    "        backbone_output_dim = 512  # domyślnie załóż 512\n",
    "    model = LinearModel(backbone, num_classes).to(device)\n",
    "    # DataLoader z normalizacją (użyjemy test_transform bo backbone wymaga takich samych normalizacji, a train_transform_supervised zawiera augmentacje które nie są potrzebne przy trenowaniu linear probe)\n",
    "    train_dataset.transform = test_transform  # stosujemy tylko normalizację do cech wejściowych backbone\n",
    "    test_dataset.transform = test_transform\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "    # Optymalizator i strata\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.fc.parameters(), lr=lr, momentum=0.9)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for (images, labels) in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)  # backbone jest zamrożony, tylko fc się uczy\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"[Linear Probing] Epoka {epoch+1}/{epochs}, średni loss: {avg_loss:.4f}\")\n",
    "    # Ewaluacja na test\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    accuracy = 100.0 * correct / total\n",
    "    print(f\"Linear probe accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Funkcja do ewaluacji k-NN (k sąsiadów) na cechach backbone\n",
    "def evaluate_knn(backbone, train_dataset, test_dataset, k=5):\n",
    "    \"\"\"\n",
    "    Ocena jakości reprezentacji za pomocą klasyfikacji k-NN.\n",
    "    Zwraca accuracy [%] na zbiorze testowym, wykorzystując k najbliższych sąsiadów z train_dataset.\n",
    "    \"\"\"\n",
    "    backbone.eval()\n",
    "    # Ekstrahujemy wszystkie cechy ze zbioru treningowego\n",
    "    train_dataset.transform = test_transform  # upewnij się, że te same przekształcenia co dla testu\n",
    "    test_dataset.transform = test_transform\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in train_loader:\n",
    "            images = images.to(device)\n",
    "            feats = backbone(images).flatten(start_dim=1)\n",
    "            # Normalizujemy cechy, aby użyć odległości kosinusowej (opcjonalnie)\n",
    "            feats = F.normalize(feats, dim=1)\n",
    "            train_features.append(feats.cpu())\n",
    "            train_labels.append(labels.cpu())\n",
    "    train_features = torch.cat(train_features, dim=0)  # [N_train, feature_dim]\n",
    "    train_labels = torch.cat(train_labels, dim=0)      # [N_train]\n",
    "    # Obliczamy cechy dla testu\n",
    "    test_features = []\n",
    "    test_labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in test_loader:\n",
    "            images = images.to(device)\n",
    "            feats = backbone(images).flatten(start_dim=1)\n",
    "            feats = F.normalize(feats, dim=1)\n",
    "            test_features.append(feats.cpu())\n",
    "            test_labels_list.append(labels.cpu())\n",
    "    test_features = torch.cat(test_features, dim=0)  # [N_test, feature_dim]\n",
    "    test_labels = torch.cat(test_labels_list, dim=0)\n",
    "    # K-NN: dla każdego testowego punktu znajdujemy k najbliższych w train_features\n",
    "    total = test_features.shape[0]\n",
    "    correct = 0\n",
    "    # Dla efektywności, liczymy w partiach\n",
    "    batch_size_eval = 100  # batch dla pętli testowej KNN, by nie przekroczyć pamięci\n",
    "    for i in range(0, total, batch_size_eval):\n",
    "        end = min(i + batch_size_eval, total)\n",
    "        test_batch = test_features[i:end]  # [batch_eval, feature_dim]\n",
    "        # Obliczamy macierz odległości (kosinusowych) między test_batch a wszystkimi train_features\n",
    "        # Ponieważ cechy są znormalizowane, odległość kosinusowa ~ 1 - cos(sim) => najbliższy = największy cos\n",
    "        # Można więc znaleźć top-k największych iloczynów skalarnych:\n",
    "        similarities = torch.matmul(test_batch, train_features.T)  # [batch_eval, N_train]\n",
    "        # Pobieramy indeksy top-k najbliższych train dla każdego test\n",
    "        topk_vals, topk_idxs = torch.topk(similarities, k, dim=1)\n",
    "        # Głosowanie większościowe\n",
    "        for idxs, true_label in zip(topk_idxs, test_labels[i:end]):\n",
    "            neigh_labels = train_labels[idxs]  # etykiety k najbliższych\n",
    "            # najczęściej występująca etykieta\n",
    "            predicted_label = torch.mode(neigh_labels).values.item()\n",
    "            if predicted_label == true_label.item():\n",
    "                correct += 1\n",
    "    accuracy = 100.0 * correct / total\n",
    "    print(f\"k-NN accuracy (k={k}): {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Ewaluacja wszystkich metod na CIFAR100 i CIFAR10\n",
    "print(\"\\n=== Ewaluacja reprezentacji (CIFAR100 i CIFAR10) ===\")\n",
    "results = {}  # słownik na wyniki accuracy\n",
    "# Linear probe i kNN dla każdej metody:\n",
    "methods = [\n",
    "    (\"MAE (masked)\", backbone_mae_cifar100),\n",
    "    (\"SimCLR\", backbone_simclr_cifar100),\n",
    "    (\"MoCo\", backbone_moco_cifar100),\n",
    "    (\"BYOL\", backbone_byol_cifar100),\n",
    "    (\"DINO\", backbone_dino_cifar100),\n",
    "    (\"Supervised\", backbone_supervised_cifar100)\n",
    "]\n",
    "for name, backbone in methods:\n",
    "    print(f\"\\nOcena dla {name}:\")\n",
    "    acc_cifar100_linear = evaluate_linear(backbone, torchvision.datasets.CIFAR100(root='./data', train=True, transform=None),\n",
    "                                         torchvision.datasets.CIFAR100(root='./data', train=False, transform=None),\n",
    "                                         num_classes=100, epochs=5, lr=0.01)\n",
    "    acc_cifar100_knn = evaluate_knn(backbone, torchvision.datasets.CIFAR100(root='./data', train=True, transform=None),\n",
    "                                    torchvision.datasets.CIFAR100(root='./data', train=False, transform=None), k=5)\n",
    "    acc_cifar10_linear = evaluate_linear(backbone, torchvision.datasets.CIFAR10(root='./data', train=True, transform=None),\n",
    "                                        torchvision.datasets.CIFAR10(root='./data', train=False, transform=None),\n",
    "                                        num_classes=10, epochs=5, lr=0.01)\n",
    "    acc_cifar10_knn = evaluate_knn(backbone, torchvision.datasets.CIFAR10(root='./data', train=True, transform=None),\n",
    "                                   torchvision.datasets.CIFAR10(root='./data', train=False, transform=None), k=5)\n",
    "    results[name] = {\n",
    "        \"CIFAR100_linear\": acc_cifar100_linear,\n",
    "        \"CIFAR100_kNN\": acc_cifar100_knn,\n",
    "        \"CIFAR10_linear\": acc_cifar10_linear,\n",
    "        \"CIFAR10_kNN\": acc_cifar10_knn\n",
    "    }\n",
    "print(\"\\n=== Podsumowanie wyników (accuracy) ===\")\n",
    "for name, res in results.items():\n",
    "    print(f\"{name}: CIFAR100 linear={res['CIFAR100_linear']:.2f}%, CIFAR100 kNN={res['CIFAR100_kNN']:.2f}%, \"\n",
    "          f\"CIFAR10 linear={res['CIFAR10_linear']:.2f}%, CIFAR10 kNN={res['CIFAR10_kNN']:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45378789",
   "metadata": {},
   "source": [
    "# 4. Klasyczne metodt reprezentacji: PCA, t-SNE, UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227042e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "    from umap import UMAP\n",
    "except ImportError:\n",
    "    UMAP = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6eacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja pomocnicza do trenowania i ewaluacji linear classifier na podanych cechach\n",
    "def train_and_eval_on_features(train_feats, train_labels, test_feats, test_labels, epochs=100, lr=0.1):\n",
    "    \"\"\"\n",
    "    Trenuje prostą regresję logistyczną (lub perceptron) na dostarczonych cechach i etykietach.\n",
    "    Zwraca accuracy na zbiorze testowym.\n",
    "    \"\"\"\n",
    "    # Zamieniamy dane na tensory PyTorch\n",
    "    X_train = torch.tensor(train_feats, dtype=torch.float32)\n",
    "    y_train = torch.tensor(train_labels, dtype=torch.long)\n",
    "    X_test = torch.tensor(test_feats, dtype=torch.float32)\n",
    "    y_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "    # Definicja prostej sieci: wejście -> num_classes (softmax w cross entropy)\n",
    "    model = nn.Linear(X_train.shape[1], len(torch.unique(y_train)))\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        # (opcjonalnie można dodać early stopping, tu pominieto dla prostoty)\n",
    "    # Ewaluacja\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test = X_test.to(device)\n",
    "        y_test = y_test.to(device)\n",
    "        outputs = model(X_test)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        acc = 100.0 * (preds == y_test).sum().item() / y_test.size(0)\n",
    "    return acc\n",
    "\n",
    "# 4.1 PCA jako metoda ekstrakcji cech\n",
    "print(\"\\n=== PCA: trenowanie na pikselach CIFAR100 ===\")\n",
    "# Przygotuj dane treningowe CIFAR100 (obrazy spłaszczone do wektora 3072)\n",
    "train_data = torchvision.datasets.CIFAR100(root='./data', train=True, transform=test_transform)\n",
    "test_data = torchvision.datasets.CIFAR100(root='./data', train=False, transform=test_transform)\n",
    "X_train = train_data.data.reshape(train_data.data.shape[0], -1) / 255.0  # normalizujemy 0-1\n",
    "y_train = np.array(train_data.targets)\n",
    "X_test = test_data.data.reshape(test_data.data.shape[0], -1) / 255.0\n",
    "y_test = np.array(test_data.targets)\n",
    "# Uczymy PCA na treningowych i transformujemy do 50 wymiarów\n",
    "pca = PCA(n_components=50)\n",
    "pca.fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(\"Wariancja wyjaśniana przez 50 składowych PCA: {:.2f}%\".format(100 * np.sum(pca.explained_variance_ratio_)))\n",
    "# Trenujemy classifier na cechach PCA\n",
    "acc_pca_cifar100 = train_and_eval_on_features(X_train_pca, y_train, X_test_pca, y_test, epochs=100, lr=0.1)\n",
    "print(f\"Accuracy klasyfikatora liniowego na 50 wymiarach PCA (CIFAR100): {acc_pca_cifar100:.2f}%\")\n",
    "\n",
    "# 4.2 t-SNE i UMAP - redukcja do 2D dla wizualizacji i ew. kNN\n",
    "print(\"\\n=== t-SNE i UMAP: wizualizacja klastrów ===\")\n",
    "# Weźmy reprezentacje wybranego modelu, np. DINO lub SimCLR, dla testowych danych CIFAR10 do wizualizacji\n",
    "backbone_for_viz = backbone_dino_cifar100  # np. backbone DINO wytrenowany na CIFAR100\n",
    "# Ekstrahujemy cechy dla wszystkich obrazów testowych CIFAR10\n",
    "test_features = []\n",
    "test_labels = []\n",
    "backbone_for_viz.eval()\n",
    "with torch.no_grad():\n",
    "    for (images, labels) in test_loader_cifar10:\n",
    "        images = images.to(device)\n",
    "        feats = backbone_for_viz(images).flatten(start_dim=1)\n",
    "        feats = feats.cpu().numpy()\n",
    "        test_features.append(feats)\n",
    "        test_labels.extend(labels.numpy())\n",
    "test_features = np.vstack(test_features)  # [10000, feature_dim]\n",
    "test_labels = np.array(test_labels)\n",
    "# t-SNE redukcja do 2 wymiarów (może chwilę potrwać)\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=42, perplexity=30)\n",
    "test_feats_2d_tsne = tsne.fit_transform(test_features[:2000])  # próbka 2000 punktów dla szybkości\n",
    "labels_sample = test_labels[:2000]\n",
    "print(\"t-SNE ukończone dla 2000 punktów.\")\n",
    "# (Opcjonalnie) UMAP redukcja do 2D\n",
    "if UMAP is not None:\n",
    "    reducer = UMAP(n_components=2, random_state=42)\n",
    "    test_feats_2d_umap = reducer.fit_transform(test_features[:2000])\n",
    "    print(\"UMAP ukończony dla 2000 punktów.\")\n",
    "# Można w tym miejscu wykonać prostą wizualizację scatter plot (np. matplotlib),\n",
    "# ale z uwagi na środowisko tekstowe, pominiemy rysowanie grafiki.\n",
    "# Zamiast tego, dokonamy oceny kNN w przestrzeni 2D t-SNE jako ciekawostkę:\n",
    "knn_acc_tsne = None\n",
    "if test_feats_2d_tsne is not None:\n",
    "    # policzmy kNN (k=5) w przestrzeni t-SNE dla próbkowanych danych\n",
    "    tree = None\n",
    "    try:\n",
    "        from sklearn.neighbors import KNeighborsClassifier\n",
    "        knn = KNeighborsClassifier(n_neighbors=5)\n",
    "        knn.fit(test_feats_2d_tsne, labels_sample)\n",
    "        preds = knn.predict(test_feats_2d_tsne)\n",
    "        knn_acc_tsne = 100.0 * np.mean(preds == labels_sample)\n",
    "        print(f\"k-NN accuracy w przestrzeni t-SNE (na próbie 2000, k=5): {knn_acc_tsne:.2f}%\")\n",
    "    except ImportError:\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
